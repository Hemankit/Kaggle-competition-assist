#!/usr/bin/env python3
"""
Comprehensive test for the Multi-Agent Component
Tests all orchestration modes and interaction patterns
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_component_orchestrator_initialization():
    """Test that the main component orchestrator initializes correctly"""
    print("üß™ Testing Component Orchestrator Initialization")
    print("=" * 50)
    
    try:
        from orchestrators.component_orchestrator import ComponentOrchestrator
        
        orchestrator = ComponentOrchestrator()
        print("‚úÖ ComponentOrchestrator initialized successfully")
        
        # Check that all sub-orchestrators are initialized
        if hasattr(orchestrator, 'langgraph_orchestrator'):
            print("‚úÖ LangGraph orchestrator initialized")
        else:
            print("‚ùå LangGraph orchestrator missing")
            
        if hasattr(orchestrator, 'multi_agent_orchestrator'):
            print("‚úÖ Multi-agent orchestrator initialized")
        else:
            print("‚ùå Multi-agent orchestrator missing")
            
        if hasattr(orchestrator, 'dynamic_orchestrator'):
            print("‚úÖ Dynamic cross-framework orchestrator initialized")
        else:
            print("‚ùå Dynamic orchestrator missing")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Component orchestrator initialization failed: {e}")
        return False

def test_dynamic_orchestrator_components():
    """Test the dynamic cross-framework orchestrator components"""
    print(f"\nüéØ Testing Dynamic Cross-Framework Orchestrator")
    print("=" * 55)
    
    try:
        from routing.dynamic_orchestrator import (
            DynamicCrossFrameworkOrchestrator,
            InteractionPattern,
            FrameworkCapability,
            AgentSelection,
            InteractionPlan
        )
        
        orchestrator = DynamicCrossFrameworkOrchestrator()
        print("‚úÖ DynamicCrossFrameworkOrchestrator initialized")
        
        # Test interaction pattern enum
        patterns = list(InteractionPattern)
        print(f"‚úÖ Interaction patterns available: {[p.value for p in patterns]}")
        
        # Test framework capability enum
        frameworks = list(FrameworkCapability)
        print(f"‚úÖ Framework capabilities available: {[f.value for f in frameworks]}")
        
        # Test agent selection dataclass
        sample_agent = AgentSelection(
            name="test_agent",
            framework="crewai",
            confidence=0.85,
            reasoning="Test reasoning",
            capabilities=["test_capability"]
        )
        print(f"‚úÖ AgentSelection dataclass works: {sample_agent.name}")
        
        # Test interaction plan dataclass
        sample_plan = InteractionPlan(
            pattern=InteractionPattern.SEQUENTIAL,
            agents=[sample_agent],
            execution_order=[0],
            expected_duration="2m",
            complexity_score=0.6
        )
        print(f"‚úÖ InteractionPlan dataclass works: {sample_plan.pattern.value}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Dynamic orchestrator component test failed: {e}")
        return False

def test_query_analysis_without_llm():
    """Test query analysis logic without making LLM calls"""
    print(f"\nüîç Testing Query Analysis Logic")
    print("=" * 35)
    
    try:
        from routing.dynamic_orchestrator import DynamicCrossFrameworkOrchestrator
        
        orchestrator = DynamicCrossFrameworkOrchestrator()
        
        # Test with mock parsed intent (simulating what LLM would return)
        mock_parsed_intent = {
            "intent": "error",
            "sub_intents": ["debug", "troubleshooting", "code_analysis"],
            "reasoning_style": "stepwise",
            "metadata_flags": {"urgency": True}
        }
        
        # Test complexity analysis
        complexity = orchestrator.analyze_query_complexity(mock_parsed_intent)
        print(f"‚úÖ Complexity analysis works: score={complexity['score']:.2f}")
        print(f"   Needs validation: {complexity['needs_validation']}")
        print(f"   Needs collaboration: {complexity['needs_collaboration']}")
        
        # Test agent selection logic
        selected_agents = orchestrator.select_agents_dynamically(mock_parsed_intent)
        print(f"‚úÖ Agent selection works: {len(selected_agents)} agents selected")
        for agent in selected_agents:
            print(f"   - {agent.name} ({agent.framework}): {agent.reasoning}")
        
        # Test interaction pattern determination
        pattern = orchestrator.determine_interaction_pattern(complexity, selected_agents)
        print(f"‚úÖ Pattern determination works: {pattern.value}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Query analysis test failed: {e}")
        return False

def test_agent_registry_integration():
    """Test that the orchestrator can access the agent registry"""
    print(f"\nüìã Testing Agent Registry Integration")
    print("=" * 40)
    
    try:
        from routing.registry import AGENT_CAPABILITY_REGISTRY, get_agent
        
        print(f"‚úÖ Agent registry loaded: {len(AGENT_CAPABILITY_REGISTRY)} agents")
        
        # Test registry contents
        for agent_name, metadata in AGENT_CAPABILITY_REGISTRY.items():
            print(f"   - {agent_name}: {metadata.get('capabilities', [])}")
        
        # Test agent instantiation (without LLM calls)
        try:
            agent = get_agent("error_diagnosis", mode="default")
            print(f"‚úÖ Agent instantiation works: {agent.__class__.__name__}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Agent instantiation failed (expected without LLM): {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Agent registry integration test failed: {e}")
        return False

def test_capability_scoring():
    """Test the capability scoring system"""
    print(f"\nüéØ Testing Capability Scoring System")
    print("=" * 40)
    
    try:
        from routing.capability_scoring import find_agents_by_subintent
        
        # Test with different subintents
        test_cases = [
            ("error_detection", "Error detection capability"),
            ("code_feedback", "Code feedback capability"),
            ("timeline_planning", "Timeline planning capability"),
            ("notebook_explanations", "Notebook explanation capability")
        ]
        
        for subintent, description in test_cases:
            matches = find_agents_by_subintent(subintent, min_score_threshold=0.1)
            print(f"‚úÖ {description}: {len(matches)} matches found")
            for match in matches[:2]:  # Show top 2 matches
                print(f"   - {match['agent']}: score={match['score']:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Capability scoring test failed: {e}")
        return False

def test_orchestrator_modes():
    """Test different orchestrator modes (without LLM calls)"""
    print(f"\nüîÑ Testing Orchestrator Modes")
    print("=" * 35)
    
    try:
        from orchestrators.component_orchestrator import ComponentOrchestrator
        
        orchestrator = ComponentOrchestrator()
        
        # Test mode validation
        test_query = "Help me debug this error"
        
        # Test invalid mode
        result = orchestrator.run({"query": test_query, "mode": "invalid_mode"})
        if "error" in result and "Unsupported mode" in result["error"]:
            print("‚úÖ Mode validation works: rejects invalid modes")
        else:
            print("‚ùå Mode validation failed")
        
        # Test that different modes exist
        valid_modes = ["dynamic", "crewai", "autogen", "langgraph"]
        for mode in valid_modes:
            # Just test that the mode is accepted (won't execute without LLM)
            try:
                result = orchestrator.run({"query": test_query, "mode": mode})
                print(f"‚úÖ Mode '{mode}' accepted by orchestrator")
            except Exception as e:
                print(f"‚ö†Ô∏è  Mode '{mode}' error (expected without LLM): {type(e).__name__}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Orchestrator modes test failed: {e}")
        return False

def test_execution_trace_system():
    """Test the execution trace and debugging system"""
    print(f"\nüìä Testing Execution Trace System")
    print("=" * 35)
    
    try:
        from orchestrators.component_orchestrator import ComponentOrchestrator
        
        orchestrator = ComponentOrchestrator()
        
        # Test debug trace methods exist
        if hasattr(orchestrator, 'get_debug_trace'):
            print("‚úÖ get_debug_trace method exists")
        else:
            print("‚ùå get_debug_trace method missing")
            
        if hasattr(orchestrator, 'run_with_debug'):
            print("‚úÖ run_with_debug method exists")
        else:
            print("‚ùå run_with_debug method missing")
        
        # Test initial trace state
        initial_trace = orchestrator.get_debug_trace()
        print(f"‚úÖ Initial trace state: {initial_trace}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Execution trace system test failed: {e}")
        return False

def test_interaction_pattern_logic():
    """Test the logic for determining interaction patterns"""
    print(f"\nüß© Testing Interaction Pattern Logic")
    print("=" * 40)
    
    try:
        from routing.dynamic_orchestrator import (
            DynamicCrossFrameworkOrchestrator,
            InteractionPattern,
            AgentSelection
        )
        
        orchestrator = DynamicCrossFrameworkOrchestrator()
        
        # Test different scenarios
        test_scenarios = [
            {
                "name": "Single Agent",
                "agents": [AgentSelection("test", "crewai", 0.8, "test", ["test"])],
                "complexity": {"needs_validation": False, "needs_collaboration": False},
                "expected": InteractionPattern.SEQUENTIAL
            },
            {
                "name": "Validation Needed",
                "agents": [
                    AgentSelection("producer", "crewai", 0.8, "test", ["code"]),
                    AgentSelection("validator", "langgraph", 0.7, "test", ["error"])
                ],
                "complexity": {"needs_validation": True, "needs_collaboration": False},
                "expected": InteractionPattern.VALIDATION
            },
            {
                "name": "Collaboration Needed",
                "agents": [
                    AgentSelection("agent1", "crewai", 0.8, "test", ["planning"]),
                    AgentSelection("agent2", "autogen", 0.7, "test", ["timeline"]),
                    AgentSelection("agent3", "langgraph", 0.6, "test", ["monitoring"])
                ],
                "complexity": {"needs_validation": False, "needs_collaboration": True},
                "expected": InteractionPattern.COLLABORATIVE
            }
        ]
        
        for scenario in test_scenarios:
            pattern = orchestrator.determine_interaction_pattern(
                scenario["complexity"], 
                scenario["agents"]
            )
            
            if pattern == scenario["expected"]:
                print(f"‚úÖ {scenario['name']}: {pattern.value} (correct)")
            else:
                print(f"‚ö†Ô∏è  {scenario['name']}: {pattern.value} (expected {scenario['expected'].value})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Interaction pattern logic test failed: {e}")
        return False

def demonstrate_multiagent_workflow():
    """Demonstrate how the multi-agent workflow would work"""
    print(f"\nüé¨ Multi-Agent Workflow Demonstration")
    print("=" * 45)
    
    try:
        from routing.dynamic_orchestrator import DynamicCrossFrameworkOrchestrator
        
        orchestrator = DynamicCrossFrameworkOrchestrator()
        
        # Simulate different query types
        demo_queries = [
            {
                "query": "I'm getting an error in my Kaggle notebook",
                "description": "Error diagnosis scenario"
            },
            {
                "query": "Help me plan my competition timeline and review my code",
                "description": "Multi-agent collaboration scenario"
            },
            {
                "query": "Explain this competition dataset",
                "description": "Simple single-agent scenario"
            }
        ]
        
        for i, demo in enumerate(demo_queries, 1):
            print(f"\nüìã Scenario {i}: {demo['description']}")
            print(f"Query: {demo['query']}")
            
            try:
                # Create interaction plan (without executing)
                plan = orchestrator.create_interaction_plan(demo['query'])
                print(f"‚úÖ Plan created:")
                print(f"   Pattern: {plan.pattern.value}")
                print(f"   Agents: {[agent.name for agent in plan.agents]}")
                print(f"   Frameworks: {list(set(agent.framework for agent in plan.agents))}")
                print(f"   Complexity: {plan.complexity_score:.2f}")
                print(f"   Duration: {plan.expected_duration}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Plan creation failed (expected without LLM): {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Workflow demonstration failed: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ Testing Multi-Agent Component")
    print("=" * 60)
    
    # Run all tests
    tests = [
        ("Component Initialization", test_component_orchestrator_initialization),
        ("Dynamic Orchestrator Components", test_dynamic_orchestrator_components),
        ("Query Analysis Logic", test_query_analysis_without_llm),
        ("Agent Registry Integration", test_agent_registry_integration),
        ("Capability Scoring", test_capability_scoring),
        ("Orchestrator Modes", test_orchestrator_modes),
        ("Execution Trace System", test_execution_trace_system),
        ("Interaction Pattern Logic", test_interaction_pattern_logic),
        ("Workflow Demonstration", demonstrate_multiagent_workflow)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} crashed: {e}")
            results[test_name] = False
    
    # Summary
    print(f"\nüìä Multi-Agent Component Test Results:")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"   {test_name}: {status}")
        if success:
            passed += 1
    
    print(f"\nüéØ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print(f"\nüéâ All multi-agent component tests passed!")
        print(f"‚úÖ The multi-agent system is ready for integration testing")
    elif passed >= total * 0.8:
        print(f"\n‚ö†Ô∏è  Most tests passed - minor issues to address")
    else:
        print(f"\n‚ùå Several tests failed - significant issues need fixing")
    
    print(f"\nüìã Next Steps:")
    print(f"   1. Fix any failed tests")
    print(f"   2. Set up API keys for LLM testing")
    print(f"   3. Test actual agent execution")
    print(f"   4. Integration testing with Flask backend")



