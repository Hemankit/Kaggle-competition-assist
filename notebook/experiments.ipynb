{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077c810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## User query handling and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98da7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install -U sentence-transformers\n",
    "%pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46c3926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import importlib.util\n",
    "import os\n",
    "# Load the SentenceTransformer model\n",
    "sent_transform_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b58c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Add the correct download path manually\n",
    "nltk_data_path = os.path.join(os.environ[\"USERPROFILE\"], \"AppData\", \"Roaming\", \"nltk_data\")\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Now download and verify\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b663ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Force redownload of punkt\n",
    "nltk_data_path = os.path.join(os.environ[\"USERPROFILE\"], \"AppData\", \"Roaming\", \"nltk_data\")\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_user_queries = [\n",
    "    \"Explain the evaluation metric used for this kaggle competition\",\n",
    "    \"What are the most efficient baseline model options given the structure of the data and the size of the dataset?\",\n",
    "    \"Given my baseline model, what are the next steps I should take to iteratively improve my model and have a submission within two or three days?\"\n",
    "]\n",
    "\n",
    "def basic_preprocess(query, remove_stopwords=False):\n",
    "    # Clean noise (URLs, code snippets, etc.)\n",
    "    cleaned_query = re.sub(r'[^\\w\\s]', '', query.lower())\n",
    "    tokens = word_tokenize(cleaned_query)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "for query in sample_user_queries:\n",
    "    tokens = basic_preprocess(query, remove_stopwords=True)\n",
    "    print(tokens)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d26193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_queries = [\n",
    "    \"Explain the evaluation metric used for this kaggle competition\",\n",
    "    \"What are the most efficient baseline model options given the structure of the data and the size of the dataset?\",\n",
    "    \"Given my baseline model, what are the next steps I should take to iteratively improve my model and have a submission within two or three days?\"\n",
    "]\n",
    "\n",
    "def improved_preprocess(query):\n",
    "    # Lowercase and remove punctuation\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', query.lower())\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Optionally spellcheck\n",
    "    # Optionally map synonyms\n",
    "    return tokens\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fdf6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis better work now!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"This better work now!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b7188c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing cleaning and preprocessing queries\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m sample_user_queries:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokens = \u001b[43mimproved_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mimproved_preprocess\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimproved_preprocess\u001b[39m(query):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Lowercase and remove punctuation\u001b[39;00m\n\u001b[32m      9\u001b[39m     cleaned = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, query.lower())\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Lemmatize\u001b[39;00m\n\u001b[32m     12\u001b[39m     lemmatizer = WordNetLemmatizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# testing cleaning and preprocessing queries\n",
    "for query in sample_user_queries:\n",
    "    tokens = improved_preprocess(query)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1daebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embeddings(query):\n",
    "    preprocessed_query = improved_preprocess(query)\n",
    "    return sent_transform_model.encode([' '.join(preprocessed_query)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418cd7ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing query embeddings\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m sample_user_queries:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     embedding = \u001b[43mquery_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding for: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery[:\u001b[32m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mquery_embeddings\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_embeddings\u001b[39m(query):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     preprocessed_query = \u001b[43mimproved_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent_transform_model.encode([\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(preprocessed_query)])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mimproved_preprocess\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimproved_preprocess\u001b[39m(query):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Lowercase and remove punctuation\u001b[39;00m\n\u001b[32m      9\u001b[39m     cleaned = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, query.lower())\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Lemmatize\u001b[39;00m\n\u001b[32m     12\u001b[39m     lemmatizer = WordNetLemmatizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\heman\\Kaggle-competition-assist\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\heman/nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\heman\\\\Kaggle-competition-assist\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\heman\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# testing query embeddings\n",
    "for query in sample_user_queries:\n",
    "    embedding = query_embeddings(query)\n",
    "    print(f\"Embedding for: \\\"{query[:60]}...\\\"\")\n",
    "    print(f\"Shape: {embedding.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95df03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent classifier\n",
    "# Use section names for section prediction\n",
    "query_sections = ['leaderboard', 'data', 'overview', 'code', 'model', 'discussion']\n",
    "\n",
    "def predict_query_section(query):\n",
    "    preprocessed_query = basic_preprocess(query)\n",
    "    query_embedding = sent_transform_model.encode([' '.join(preprocessed_query)])\n",
    "    section_embedding = sent_transform_model.encode(query_sections)\n",
    "    section_similarities = cosine_similarity(query_embedding, section_embedding)[0]\n",
    "    predicted_section = query_sections[section_similarities.argmax()]\n",
    "    return predicted_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing intent prediction\n",
    "for query in sample_user_queries:\n",
    "    action, topic = predict_query_section(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"→ Predicted Action: {action}\")\n",
    "    print(f\"→ Predicted Topic: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def debug_intent_prediction(query):\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    query_embedding = sent_transform_model.encode([' '.join(preprocessed_query)])\n",
    "    \n",
    "    action_embedding = sent_transform_model.encode(query_intent_actions)\n",
    "    topic_embedding = sent_transform_model.encode(query_intent_topics)\n",
    "    \n",
    "    action_sim = cosine_similarity(query_embedding, action_embedding)[0]\n",
    "    topic_sim = cosine_similarity(query_embedding, topic_embedding)[0]\n",
    "    \n",
    "    pprint({\n",
    "        'query': query,\n",
    "        'action_scores': dict(zip(query_intent_actions, action_sim)),\n",
    "        'topic_scores': dict(zip(query_intent_topics, topic_sim)),\n",
    "    })\n",
    "\n",
    "    debug_intent_prediction(sample_user_queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query classification and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e43f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_pipeline(query, query_intent_actions, query_intent_topics):\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/roberta-large-mnli\")\n",
    "    \n",
    "    # Classify action\n",
    "    action_result = classifier(query, candidate_labels=query_intent_actions)\n",
    "    predicted_action = action_result['labels'][0]\n",
    "    \n",
    "    # Classify topic\n",
    "    topic_result = classifier(query, candidate_labels=query_intent_topics)\n",
    "    predicted_topic = topic_result['labels'][0]\n",
    "    \n",
    "    return predicted_action, predicted_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "def evaluate_zero_shot_classifier(queries, true_actions, true_topics):\n",
    "    predicted_actions = []\n",
    "    predicted_topics = []\n",
    "    \n",
    "    for query in queries:\n",
    "        action, topic = zero_shot_pipeline(query, query_intent_actions, query_intent_topics)\n",
    "        predicted_actions.append(action)\n",
    "        predicted_topics.append(topic)\n",
    "    \n",
    "    action_loss = hamming_loss(true_actions, predicted_actions)\n",
    "    topic_loss = hamming_loss(true_topics, predicted_topics)\n",
    "    \n",
    "    return action_loss, topic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d3933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca018807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the simple routing signaling\n",
    "sample_user_queries = [\n",
    "    \"Explain the evaluation metric used for this Kaggle competition\",\n",
    "    \"What are the most efficient baseline model options?\",\n",
    "    \"Where can I find the leaderboard?\",\n",
    "    \"Can you summarize the competition timeline?\",\n",
    "    \"Here is a screenshot of the discussion — what does it say?\"\n",
    "]\n",
    "\n",
    "for query in sample_user_queries:\n",
    "    print(f\"\\n🧪 Testing query: {query}\")\n",
    "    response = handle_query(query)\n",
    "    print(f\"🔁 Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experimenting with building model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9343282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid scraper and router agent (infomation retrieval layer)\n",
    "# Custom built (no langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "952bfab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapegraphai in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (1.58.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (5.0.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (4.13.4)\n",
      "Requirement already satisfied: duckduckgo-search>=7.2.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (8.0.4)\n",
      "Requirement already satisfied: free-proxy>=1.1.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (1.1.3)\n",
      "Requirement already satisfied: html2text>=2024.2.26 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (2025.4.15)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (4.24.0)\n",
      "Requirement already satisfied: langchain-aws>=0.1.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.2.26)\n",
      "Requirement already satisfied: langchain-community>=0.2.9 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.3.26)\n",
      "Requirement already satisfied: langchain-mistralai>=0.1.12 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.2.10)\n",
      "Requirement already satisfied: langchain-ollama>=0.1.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.3.3)\n",
      "Requirement already satisfied: langchain-openai>=0.1.22 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.3.24)\n",
      "Requirement already satisfied: langchain>=0.3.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.3.26)\n",
      "Requirement already satisfied: minify-html>=0.15.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.16.4)\n",
      "Requirement already satisfied: playwright>=1.43.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (1.52.0)\n",
      "Requirement already satisfied: pydantic>=2.10.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (1.1.0)\n",
      "Requirement already satisfied: scrapegraph-py>=0.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (1.12.0)\n",
      "Requirement already satisfied: semchunk>=2.2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (3.2.2)\n",
      "Requirement already satisfied: simpleeval>=1.0.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (1.0.3)\n",
      "Requirement already satisfied: tiktoken>=0.7 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.66.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (4.67.1)\n",
      "Requirement already satisfied: undetected-playwright>=0.3.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from scrapegraphai) (0.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from beautifulsoup4>=4.12.3->scrapegraphai) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from beautifulsoup4>=4.12.3->scrapegraphai) (4.13.2)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from duckduckgo-search>=7.2.1->scrapegraphai) (8.2.1)\n",
      "Requirement already satisfied: primp>=0.15.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from duckduckgo-search>=7.2.1->scrapegraphai) (0.15.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from duckduckgo-search>=7.2.1->scrapegraphai) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from click>=8.1.8->duckduckgo-search>=7.2.1->scrapegraphai) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from free-proxy>=1.1.1->scrapegraphai) (2.32.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonschema>=4.23.0->scrapegraphai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonschema>=4.23.0->scrapegraphai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonschema>=4.23.0->scrapegraphai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonschema>=4.23.0->scrapegraphai) (0.25.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain>=0.3.0->scrapegraphai) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain>=0.3.0->scrapegraphai) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain>=0.3.0->scrapegraphai) (0.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain>=0.3.0->scrapegraphai) (2.0.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain>=0.3.0->scrapegraphai) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.3.0->scrapegraphai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.3.0->scrapegraphai) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.3.0->scrapegraphai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.3.0->scrapegraphai) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic>=2.10.2->scrapegraphai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic>=2.10.2->scrapegraphai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic>=2.10.2->scrapegraphai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->free-proxy>=1.1.1->scrapegraphai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->free-proxy>=1.1.1->scrapegraphai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->free-proxy>=1.1.1->scrapegraphai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests->free-proxy>=1.1.1->scrapegraphai) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3.0->scrapegraphai) (3.2.3)\n",
      "Requirement already satisfied: boto3>=1.37.24 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-aws>=0.1.3->scrapegraphai) (1.38.41)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-aws>=0.1.3->scrapegraphai) (2.3.1)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.41 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from boto3>=1.37.24->langchain-aws>=0.1.3->scrapegraphai) (1.38.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from boto3>=1.37.24->langchain-aws>=0.1.3->scrapegraphai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from boto3>=1.37.24->langchain-aws>=0.1.3->scrapegraphai) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from botocore<1.39.0,>=1.38.41->boto3>=1.37.24->langchain-aws>=0.1.3->scrapegraphai) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.41->boto3>=1.37.24->langchain-aws>=0.1.3->scrapegraphai) (1.17.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-community>=0.2.9->scrapegraphai) (3.12.13)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-community>=0.2.9->scrapegraphai) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-community>=0.2.9->scrapegraphai) (2.10.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-community>=0.2.9->scrapegraphai) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.9->scrapegraphai) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.9->scrapegraphai) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.9->scrapegraphai) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.9->scrapegraphai) (1.1.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-mistralai>=0.1.12->scrapegraphai) (0.21.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-mistralai>=0.1.12->scrapegraphai) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai>=0.1.12->scrapegraphai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai>=0.1.12->scrapegraphai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai>=0.1.12->scrapegraphai) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai>=0.1.12->scrapegraphai) (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai>=0.1.12->scrapegraphai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai>=0.1.12->scrapegraphai) (2025.5.1)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.4.8 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-ollama>=0.1.3->scrapegraphai) (0.5.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-openai>=0.1.22->scrapegraphai) (1.90.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.1.22->scrapegraphai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.1.22->scrapegraphai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.1.22->scrapegraphai) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from tiktoken>=0.7->scrapegraphai) (2024.11.6)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.3.0->scrapegraphai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.3.0->scrapegraphai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.3.0->scrapegraphai) (0.23.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from playwright>=1.43.0->scrapegraphai) (13.0.0)\n",
      "Requirement already satisfied: mpire[dill] in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from semchunk>=2.2.0->scrapegraphai) (2.10.2)\n",
      "Requirement already satisfied: pygments>=2.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from mpire[dill]->semchunk>=2.2.0->scrapegraphai) (2.19.1)\n",
      "Requirement already satisfied: pywin32>=301 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from mpire[dill]->semchunk>=2.2.0->scrapegraphai) (310)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from mpire[dill]->semchunk>=2.2.0->scrapegraphai) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk>=2.2.0->scrapegraphai) (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: groq in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\heman\\kaggle-competition-assist\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapegraphai\n",
    "%pip install langchain\n",
    "%pip install langchain groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd84e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Kaggle_Fetcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Kaggle_Fetcher imports\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mKaggle_Fetcher\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkaggle_api_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     get_notebooks_info,\n\u001b[32m      4\u001b[39m     get_leaderboard_info,\n\u001b[32m      5\u001b[39m     get_dataset_metadata,\n\u001b[32m      6\u001b[39m     get_total_notebooks_count,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mKaggle_Fetcher\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkaggle_fetcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleFetcher\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Scraper imports\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Kaggle_Fetcher'"
     ]
    }
   ],
   "source": [
    "# Kaggle_Fetcher imports\n",
    "from Kaggle_Fetcher.kaggle_api_client import (\n",
    "    get_notebooks_info,\n",
    "    get_leaderboard_info,\n",
    "    get_dataset_metadata,\n",
    "    get_total_notebooks_count,\n",
    ")\n",
    "from Kaggle_Fetcher.kaggle_fetcher import KaggleFetcher\n",
    "\n",
    "# Scraper imports\n",
    "from scraper.notebook_scraper import ScrapingAndStructuringNotebookMetadata\n",
    "from scraper.model_scraper import ScrapingModelSectionData\n",
    "from scraper.code_scraper import CodeParser\n",
    "from scraper.scrape_handlers import scrapegraphai_handler, select_urls_from_metadata\n",
    "from scraper.screenshots_handler import (\n",
    "    download_image,\n",
    "    perform_ocr,\n",
    "    extract_text_from_posts,\n",
    ")\n",
    "from scraper.kaggle_scraper import ScraperOrchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "# Fix: Correct import from 'langchain', not 'lanchain'\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import tool\n",
    "from trio import TooSlowError\n",
    "\n",
    "\n",
    "class HybridScraperRouterAgent:\n",
    "    def __init__(self, competition_name, sent_transform_model):\n",
    "        self.competition_name = competition_name\n",
    "        self.kaggle_fetcher = KaggleFetcher()\n",
    "        self.scraper_orchestrator = ScraperOrchestrator(\"competition-name\")\n",
    "        self.chat_model = ChatGroq(model=\"groq/llama-3.1-70b-instruct\")\n",
    "        self.sent_transform_model = sent_transform_model\n",
    "        # ... The above initializations is for our first feature for agent\n",
    "    SECTION_KEYWORDS = {\n",
    "        \"leaderboard\": [\"leaderboard\", \"current rank\", \"previous rank\", \"top score\", \"entries\"],\n",
    "        \"data\": [\"data\", \"data information\", \"download data\"],\n",
    "        \"overview\" : [\"overview\", \"evaluation metric\", \"timeline\", \"competition description\"],\n",
    "        \"code\" : [\"code\", \"notebook\", \"score\", \"runtime\"],\n",
    "        \"model\" : [\"model\", \"baseline\", \"architecture\", \"usage guide\"],\n",
    "        \"discussion\": [\"discussion\", \"forum\", \"question\", \"post\"],\n",
    "    }\n",
    "    SECTION_TO_METHOD = {\n",
    "        \"leaderboard\": \"api\",\n",
    "        \"data\": \"api\",\n",
    "        \"overview\": \"api\",\n",
    "        \"code\": \"scraper\",\n",
    "        \"model\": \"scraper\",\n",
    "        \"discussion\": \"scraper\",\n",
    "    }\n",
    "    def detect_section_select_retrieval(self, query):\n",
    "        # directly checking if keyword is present in user query\n",
    "        for section, keywords in self.SECTION_KEYWORDS.items():\n",
    "            if any(keyword in query.lower() for keyword in keywords):\n",
    "                return section\n",
    "        # indirect checking using embeddings and cosine similarity\n",
    "        _, predicted_section = predict_query_intent(query)\n",
    "        return predicted_section\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9179250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b386fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain core building blocks\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.agents import AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# For tool-based routing or filtering\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# If you're going to classify query intent or section match\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "# For using a specific LLM model, e.g., Groq\n",
    "from langchain.chat_models import ChatGroq\n",
    "\n",
    "# Optional: if you’ll build a tool picker or retriever\n",
    "from langchain.agents import Tool\n",
    "\n",
    "\n",
    "# imports from scraper and api handling folders\n",
    "from Kaggle_Fetcher.kaggle_fetcher import KaggleFetcher\n",
    "from scraper.overview_scraper import OverviewScraper\n",
    "from scraper.notebook_scraper_v2 import NotebookScraperV2\n",
    "from scraper.model_scraper_v2 import ModelScraperV2\n",
    "from scraper.discussion_scraper_v2 import DiscussionScraperV2\n",
    "from scraper.ai_scrape_config import AIScrapeConfig\n",
    "from scraper.scrape_handlers import scrapegraphai_handler\n",
    "from scraper.screenshots_handler import extract_text_from_posts\n",
    "\n",
    "ai_config = AIScrapeConfig(\"ai_scrape_config.json\")\n",
    "ai_config.trigger_deep_scraping(\"summarize_top_notebooks\")\n",
    "\n",
    "import logging\n",
    "from logging import Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set up the logger\n",
    "logger.setLevel(logging.INFO)\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for using pydantic model\n",
    "class RetrievalResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    section: str\n",
    "    relevance_score: Optional[float]\n",
    "    metadata: Optional[dict]\n",
    "\n",
    "# Pydantic schemas for deep_scrape structured results\n",
    "\n",
    "class pydantic_notebook_deep_scrape_schema(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    section: str\n",
    "    relevance_score: Optional[float]\n",
    "    metadata: dict = Field(\n",
    "        description=\"Metadata about the notebook, including score, runtime, upvotes, and date.\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"Full content of the notebook after deep scraping.\"\n",
    "    )\n",
    "\n",
    "class pydantic_discussion_deep_scrape_schema(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    section: str\n",
    "    relevance_score: Optional[float]\n",
    "    metadata: dict = Field(\n",
    "        description=\"Metadata about the discussion post, including upvotes and date.\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"Full content of the discussion post after deep scraping.\"\n",
    "    )\n",
    "\n",
    "class pydantic_model_deep_scrape_schema(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    section: str\n",
    "    relevance_score: Optional[float]\n",
    "    metadata: dict = Field(\n",
    "        description=\"Metadata about the model, including score and usability.\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"Full content of the model description after deep scraping.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class HybridScraperRouterAgentV1:\n",
    "   def __init__(self, competition_name, sent_transform_model, ai_config_path=\"ai_scrape_config.json\"):\n",
    "    self.overview_scraper = OverviewScraper(competition_name)\n",
    "    self.notebook_scraper = NotebookScraperV2(competition_name)\n",
    "    self.model_scraper = ModelScraperV2(competition_name)\n",
    "    self.discussion_scraper = DiscussionScraperV2(competition_name)\n",
    "    self.kaggle_api = KaggleFetcher()\n",
    "\n",
    "    self.models = self.model_scraper.get_all_cleaned_models()\n",
    "    self.sent_transform_model = sent_transform_model\n",
    "    self.chat_model = ChatGroq(model=\"mixtral-8x7b\", temperature=0.4)\n",
    "\n",
    "    self.ocr_handler = extract_text_from_posts\n",
    "    self.deep_scraper_config = AIScrapeConfig(config_path=ai_config_path)\n",
    "    self.deep_scrape_decider = self.should_deep_scrape_chain()\n",
    "    self.cache = {}\n",
    "        # -----------------------------------------------------step 1 key initialization-------------------------------------\n",
    "                                # Decision layer for routing user queries to the appropriate retrieval method\n",
    "    # method to detect section based on user query\n",
    "def section_detection(self, query):\n",
    "    SECTION_KEYWORDS = {\n",
    "        \"leaderboard\": [\"leaderboard\", \"current rank\", \"previous rank\", \"top score\", \"entries\"],\n",
    "        \"data\": [\"data\", \"data information\", \"download data\"],\n",
    "        \"overview\": [\"overview\", \"evaluation metric\", \"timeline\", \"competition description\"],\n",
    "        \"code\": [\"code\", \"notebook\", \"score\", \"runtime\"],\n",
    "        \"model\": [\"model\", \"baseline\", \"architecture\", \"usage guide\"],\n",
    "        \"discussion\": [\"discussion\", \"forum\", \"question\", \"post\"],\n",
    "        }\n",
    "    \"\"\"\n",
    "        Detect the section based on the user query.\n",
    "        Uses both keyword matching and embedding similarity.\n",
    "        \"\"\"\n",
    "        # Direct keyword matching\n",
    "    for section, keywords in SECTION_KEYWORDS.items():\n",
    "        if any(keyword in query.lower() for keyword in keywords):\n",
    "            return section\n",
    "        \n",
    "        # Indirect matching using embeddings\n",
    "\n",
    "        _, predicted_section = predict_query_section(query)\n",
    "        return predicted_section\n",
    "    \n",
    "    \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def route_to_retrieval_method(self, section, predicted_section): \n",
    "    \"\"\"\n",
    "    Route to the appropriate retrieval method based on the section.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Routing query. Queried section: '{section}', Predicted section: '{predicted_section}'\")\n",
    "\n",
    "    # Error handling for mismatched sections\n",
    "    if section != predicted_section:\n",
    "        logger.error(f\"Section mismatch: queried='{section}' vs predicted='{predicted_section}'\")\n",
    "        raise ValueError(f\"Section mismatch: {section} != {predicted_section}\")\n",
    "\n",
    "    # Define available retrievers\n",
    "    api_retriever = {\n",
    "        \"leaderboard\": self.kaggle_api.fetch_leaderboard_metadata,\n",
    "        \"data\": self.kaggle_api.fetch_dataset_metadata,\n",
    "    }\n",
    "    scraper_retriever = {\n",
    "        \"overview\": self.overview_scraper.scrape,\n",
    "        \"code\": self.notebook_scraper.scrape,\n",
    "        \"model\": self.model_scraper.scrape_models,\n",
    "        \"discussion\": self.discussion_scraper.scrape,\n",
    "    }\n",
    "\n",
    "    # Choose and invoke the retrieval method\n",
    "    if section in api_retriever:\n",
    "        logger.info(f\"Using API retriever for section: '{section}'\")\n",
    "        result = api_retriever[section]()\n",
    "\n",
    "    # Store result with section tag\n",
    "        self.api_results.append({\n",
    "            \"section\": section,\n",
    "            \"retrieved_at\": datetime.now().isoformat(),\n",
    "            \"content\": result\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Using scraper retriever for section: '{section}'\")\n",
    "    result = scraper_retriever[section]()\n",
    "    return result\n",
    "    \n",
    "        # ----------------------------------------------------step 2 tool and filtering/sorting results-------------------------------\n",
    "                                                                # baseline intelligent retrieval layer\n",
    "# tool wrapping\n",
    "def retriever_tools(self) -> list[Tool]:\n",
    "        \"\"\"\n",
    "        Create tools for each retrieval method.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            tool(\n",
    "                name=\"Get Leaderboard Info\",\n",
    "                func=self.kaggle_api.fetch_leaderboard_metadata,\n",
    "                description=\"Retrieve the current leaderboard information for the Kaggle competition.\"\n",
    "            ),\n",
    "            tool(\n",
    "                name=\"Get Dataset Metadata\",\n",
    "                func=self.kaggle_api.fetch_dataset_metadata,\n",
    "                description=\"Retrieve metadata about the dataset used in the Kaggle competition.\"\n",
    "            ),\n",
    "            tool(\n",
    "                name=\"Scrape Overview Section\",\n",
    "                func=self.overview_scraper.scrape,\n",
    "                description=\"Scrape the overview section of the Kaggle competition, including evaluation metrics and timeline.\"\n",
    "            ),\n",
    "            tool(\n",
    "                name=\"Scrape Code Section\",\n",
    "                func=self.notebook_scraper.scrape,\n",
    "                description=\"Scrape the code section of the Kaggle competition, including notebooks and their metadata.\"\n",
    "            ),\n",
    "            tool(\n",
    "                name=\"Scrape Model Section\",\n",
    "                func=self.model_scraper.scrape_models,\n",
    "                description=\"Scrape the model section of the Kaggle competition, including baseline models and architecture.\"\n",
    "            ),\n",
    "            tool(\n",
    "                name=\"Scrape Discussion Section\",\n",
    "                func=self.discussion_scraper.scrape,\n",
    "                description=\"Scrape the discussion section of the Kaggle competition, including posts and comments.\"\n",
    "            )\n",
    "        ]\n",
    "# caching\n",
    "def get_cache(self, content_hash: str):\n",
    "    \"\"\"\n",
    "    Get cached data for a specific content hash.\n",
    "    Returns None if no cache exists for the hash.\n",
    "    \"\"\"\n",
    "    return self.cache.get(content_hash)\n",
    "\n",
    "def update_cache(self, content_hash: str, deep_scraped: bool):\n",
    "    \"\"\"\n",
    "    Update cache with deep scraping status.\n",
    "    \"\"\"\n",
    "    self.cache[content_hash] = {\n",
    "        \"deep_scraped\": deep_scraped,\n",
    "        \"deep_scraped_at\": datetime.now(datetime.timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# sorting and filtering results based on metadata\n",
    "def filter_and_sort_results(self, results, section):\n",
    "    \"\"\"\n",
    "    Filter and sort results based on metadata.\n",
    "    \"\"\"\n",
    "    Logger.info(f\"Filtering and sorting results for section: '{section}'\")\n",
    "\n",
    "    def check_required_keys(item, keys, item_type=\"entry\"):\n",
    "        for key in keys:\n",
    "            if key not in item:\n",
    "                Logger.warning(f\"{item_type} missing key '{key}': {item}\")\n",
    "\n",
    "    pinned_notebooks, other_notebooks = [], []\n",
    "    pinned_posts, other_posts = [], []\n",
    "    filtered_results = []\n",
    "\n",
    "    if section == \"code\":\n",
    "        pinned_notebooks = [nb for nb in results if \"pinned\" in nb.get(\"section\", \"\").lower()]\n",
    "        other_notebooks = [nb for nb in results if \"unpinned\" in nb.get(\"section\", \"\").lower()]\n",
    "\n",
    "        for nb in results:\n",
    "            check_required_keys(nb, [\"votes\", \"date_created\", \"notebook_hash\"], item_type=\"notebook\")\n",
    "\n",
    "        pinned_notebooks.sort(\n",
    "            key=lambda x: (\n",
    "                x.get(\"notebook_score\", 0),\n",
    "                x.get(\"votes\", 0),\n",
    "                x.get(\"date_created\", \"\")\n",
    "            ),\n",
    "            reverse=True\n",
    "        )\n",
    "        other_notebooks.sort(\n",
    "            key=lambda x: (\n",
    "                x.get(\"notebook_score\", 0),\n",
    "                x.get(\"votes\", 0),\n",
    "                x.get(\"date_created\", \"\")\n",
    "            ),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "    elif section == \"discussion\":\n",
    "        pinned_posts = [post for post in results if post.get(\"pinned\", False)]\n",
    "        other_posts = [post for post in results if not post.get(\"pinned\", False)]\n",
    "\n",
    "        for post in results:\n",
    "            check_required_keys(post, [\"upvotes\", \"date\"], item_type=\"discussion_post\")\n",
    "\n",
    "        pinned_posts.sort(key=lambda x: (x.get(\"upvotes\", 0), x.get(\"date\", \"\")), reverse=True)\n",
    "        other_posts.sort(key=lambda x: (x.get(\"upvotes\", 0), x.get(\"date\", \"\")), reverse=True)\n",
    "\n",
    "    elif section == \"model\":\n",
    "        filtered_results = [\n",
    "            model for model in results\n",
    "            if model.get(\"score\", 0) > 0.5 and model.get(\"usability\", 0) > 0.5\n",
    "        ]\n",
    "\n",
    "        for model in results:\n",
    "            check_required_keys(model, [\"score\", \"usability\"], item_type=\"model_entry\")\n",
    "\n",
    "        filtered_results.sort(\n",
    "            key=lambda x: (x.get(\"score\", 0), x.get(\"usability\", 0)),\n",
    "            reverse=True\n",
    "        )\n",
    "        # Cache check below applies only to `filtered_results`\n",
    "        for result in filtered_results:\n",
    "            content_hash = result.get(\"content_hash\")\n",
    "            cached_data = self.get_cache(content_hash)\n",
    "            result[\"deep_scraped\"] = cached_data.get(\"deep_scraped\", False) if cached_data else False\n",
    "            result[\"deep_scraped_at\"] = cached_data.get(\"deep_scraped_at\") if cached_data else None\n",
    "        return filtered_results\n",
    "\n",
    "    else:\n",
    "        Logger.warning(f\"No filtering rules defined for section: {section}\")\n",
    "        return results\n",
    "\n",
    "    # Perform deep_scraped cache check for code and discussion sections\n",
    "    combined = pinned_notebooks + other_notebooks + pinned_posts + other_posts\n",
    "    for result in combined:\n",
    "        content_hash = result.get(\"notebook_hash\") or result.get(\"content_hash\")\n",
    "        cached_data = self.get_cache(content_hash)\n",
    "        result[\"deep_scraped\"] = cached_data.get(\"deep_scraped\", False) if cached_data else False\n",
    "        result[\"deep_scraped_at\"] = cached_data.get(\"deep_scraped_at\") if cached_data else None\n",
    "\n",
    "    return combined\n",
    "    \n",
    "        \n",
    "    \n",
    "    # relevance filtering\n",
    "def relevance_filtering(self, results, query, section):\n",
    "    \"\"\"\n",
    "    Filter results based on relevance to the user query and optionally sort them by similarity.\n",
    "    \"\"\"\n",
    "    Logger.info(f\"Filtering results for section: '{section}' based on query relevance\")\n",
    "\n",
    "    query_embedding = self.sent_transform_model.encode([query])\n",
    "    filtered_results = []\n",
    "\n",
    "    for result in results:\n",
    "        if section == \"code\":\n",
    "            content = \"\\n\\n\".join(result.get(\"markdown_blocks\", []))\n",
    "        elif section == \"discussion\":\n",
    "            content = result.get(\"text\", \"\")\n",
    "        elif section == \"model\":\n",
    "            content = result.get(\"description\", \"\")\n",
    "        else:\n",
    "            content = \"\"\n",
    "\n",
    "        if content:\n",
    "            content_embedding = self.sent_transform_model.encode([content])\n",
    "            similarity = cosine_similarity(query_embedding, content_embedding)[0][0]\n",
    "\n",
    "            if similarity > 0.5:  # Relevance threshold\n",
    "                result[\"relevance_score\"] = similarity\n",
    "                filtered_results.append(result)\n",
    "\n",
    "    if filtered_results:\n",
    "        Logger.info(f\"Filtered {len(filtered_results)} relevant results for section: '{section}'\")\n",
    "        # 🔹 Fallback: sort by similarity in descending order\n",
    "        filtered_results.sort(key=lambda x: x.get(\"relevance_score\", 0), reverse=True)\n",
    "    else:\n",
    "        Logger.warning(f\"No relevant results found for query in section: '{section}'\")\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "def compute_content_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# structuring the filtered and sorted results using pydantic model\n",
    "def structuring_filtered_and_sorted_results(self, filtered_results, section):\n",
    "    \"\"\"\n",
    "    Structure the filtered and sorted results into a consistent format.\n",
    "    \"\"\"\n",
    "    pydantic_results = []\n",
    "    logger.info(f\"Structuring {len(filtered_results)} results for section: '{section}'\")\n",
    "    if not filtered_results:\n",
    "        logger.warning(f\"No results to structure for section: '{section}'\")\n",
    "        return pydantic_results\n",
    "    for result in filtered_results:\n",
    "        if section == \"code\":\n",
    "            pydantic_result = RetrievalResult(\n",
    "            title=result.get(\"title\", \"\"),\n",
    "            url=result.get(\"url\", \"\"),\n",
    "            section=section,\n",
    "            relevance_score=result.get(\"relevance_score\", None),\n",
    "            metadata={\n",
    "                \"notebook_hash\": result.get(\"notebook_hash\", \"\"),\n",
    "                \"last_scraped\": result.get(\"last_scraped\", \"\"),\n",
    "                \"votes\": result.get(\"votes\", 0),\n",
    "                \"notebook_score\": result.get(\"notebook_score\", 0),\n",
    "                \"date_created\": result.get(\"date_created\", \"\"),\n",
    "        }\n",
    "    )\n",
    "        elif section == \"discussion\":\n",
    "            pydantic_result = RetrievalResult(\n",
    "                title=result.get(\"title\", \"\"),\n",
    "                url=result.get(\"url\", \"\"),\n",
    "                section=section,\n",
    "                relevance_score=result.get(\"relevance_score\", None),\n",
    "                metadata={\n",
    "                    \"upvotes\": result.get(\"upvotes\", 0),\n",
    "                    \"date\": result.get(\"date\", \"\")\n",
    "                }\n",
    "            )\n",
    "        elif section == \"model\":\n",
    "            pydantic_result = RetrievalResult(\n",
    "                title=result.get(\"title\", \"\"),\n",
    "                url=result.get(\"url\", \"\"),\n",
    "                section=section,\n",
    "                relevance_score=result.get(\"relevance_score\", None),\n",
    "                metadata={\n",
    "                    \"score\": result.get(\"score\", 0),\n",
    "                    \"usability\": result.get(\"usability\", 0)\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            pydantic_result = RetrievalResult(\n",
    "                title=result.get(\"title\", \"\"),\n",
    "                url=result.get(\"url\", \"\"),\n",
    "                section=section,\n",
    "                relevance_score=None,\n",
    "                metadata={}\n",
    "            )\n",
    "\n",
    "        \n",
    "        structured_dict = pydantic_result.dict()\n",
    "        raw_for_hash = f\"{structured_dict['title']}|{structured_dict['url']}\"\n",
    "        structured_dict.update({\n",
    "    \"content_hash\": compute_content_hash(raw_for_hash),\n",
    "    \"deep_scraped\": False,\n",
    "    \"deep_scraped_at\": None,\n",
    "})\n",
    "\n",
    "        pydantic_results.append(structured_dict)\n",
    "        \n",
    "    \n",
    "    # -------------------------------------------step 3 conditional deep scraping --------------------------------------------------\n",
    "                                                # Intelligent scraping decision layer\n",
    "\n",
    "def initialize_ocr_handler(self):\n",
    "    return extract_text_from_posts\n",
    "\n",
    "def load_deep_scraping_config(self):\n",
    "    return AIScrapeConfig().get_scraping_preferences()\n",
    "\n",
    "def should_deep_scrape_chain(self):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert content retriever. Based on the user’s query and the content metadata, decide if a deeper scrape is needed.\"),\n",
    "        (\"human\", \"\"\"User query: {query}\n",
    "Section: {section}\n",
    "Item title: {title}\n",
    "Has image: {has_image}\n",
    "Is pinned: {pinned}\n",
    "Brief content: {content_snippet}\n",
    "\n",
    "Should this be deep scraped? Answer YES or NO and briefly explain.\"\"\")\n",
    "    ])\n",
    "    return prompt | self.chat_model | StrOutputParser()\n",
    "\n",
    "def deep_scraping_pinned(self, query, section):\n",
    "    \"\"\"\n",
    "    Perform deep scraping for pinned notebooks or discussion posts based on the section.\n",
    "    \"\"\"\n",
    "    if section == \"code\":\n",
    "        pinned_items = [n for n in self.notebook_scraper.get_all_cleaned_notebooks() if n.get(\"pinned\", False)]\n",
    "        scraper = self.notebook_scraper.deep_scrape_notebooks\n",
    "    elif section == \"discussion\":\n",
    "        pinned_items = [d for d in self.discussion_scraper.load_from_json() if d.get(\"pinned\", False)]\n",
    "        scraper = self.discussion_scraper.deep_scrape_discussion\n",
    "    elif section == \"model\":\n",
    "        pinned_items = [m for m in self.model_scraper.load_from_json() if m.get(\"pinned\", False)]\n",
    "        scraper = self.model_scraper.deep_scrape_model_section\n",
    "    else:\n",
    "        logger.warning(f\"No pinned deep scraping logic defined for section: {section}\")\n",
    "        return []\n",
    "\n",
    "    if pinned_items:\n",
    "        logger.info(f\"Deep scraping pinned items for query: '{query}' in section: '{section}'\")\n",
    "        deep_scraped = scraper(pinned_items)\n",
    "        for item in deep_scraped:\n",
    "            content_hash = compute_content_hash(item.get(\"content\", \"\"))\n",
    "            self.update_cache(content_hash, deep_scraped=True)\n",
    "        return deep_scraped\n",
    "    else:\n",
    "        logger.warning(f\"No pinned items found for deep scraping in section: {section}\")\n",
    "        return []\n",
    "\n",
    "def deep_scraping_not_pinned(self, query, section, llm):\n",
    "    section_metadata = self.get_section_metadata(section)\n",
    "\n",
    "    if section == \"code\":\n",
    "        items = [n for n in self.notebook_scraper.get_all_cleaned_notebooks() if not n.get(\"pinned\", False)]\n",
    "        scraper = self.notebook_scraper.deep_scrape_notebooks\n",
    "    elif section == \"discussion\":\n",
    "        items = [d for d in self.discussion_scraper.load_from_json() if not d.get(\"pinned\", False)]\n",
    "        scraper = self.discussion_scraper.deep_scrape_discussion\n",
    "    elif section == \"model\":\n",
    "        items = [m for m in self.model_scraper.load_from_json() if not m.get(\"pinned\", False)]\n",
    "        scraper = self.model_scraper.deep_scrape_model_section\n",
    "    else:\n",
    "        logger.warning(f\"No deep scraping logic defined for section: {section}\")\n",
    "        return []\n",
    "\n",
    "    deep_scraped_results = []\n",
    "\n",
    "    for item in items:\n",
    "        content_hash = compute_content_hash(item.get(\"content\", \"\"))\n",
    "        cached_data = self.get_cache(content_hash)\n",
    "\n",
    "        if cached_data and cached_data[\"deep_scraped\"]:\n",
    "            logger.info(f\"Skipping already scraped item: {item.get('title', '')}\")\n",
    "            continue\n",
    "\n",
    "        decision = self.deep_scrape_decider.invoke({\n",
    "            \"query\": query,\n",
    "            \"section\": section,\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"has_image\": item.get(\"has_image\", False),\n",
    "            \"pinned\": item.get(\"pinned\", False),\n",
    "            \"content_snippet\": item.get(\"content_snippet\", \"\")\n",
    "        })\n",
    "\n",
    "        if \"YES\" in decision.upper():\n",
    "            logger.info(f\"Deep scraping item: {item.get('title', '')}\")\n",
    "            result = scraper([item])\n",
    "            deep_scraped_results.extend(result)\n",
    "            for r in result:\n",
    "                result_hash = compute_content_hash(r.get(\"content\", \"\"))\n",
    "                self.update_cache(result_hash, deep_scraped=True)\n",
    "        else:\n",
    "            logger.info(f\"No deep scraping needed for item: {item.get('title', '')}\")\n",
    "\n",
    "    return deep_scraped_results\n",
    "\n",
    "def get_section_metadata(self, section: str):\n",
    "    if section == \"code\":\n",
    "        return self.notebook_scraper.get_all_cleaned_notebooks()\n",
    "    elif section == \"discussion\":\n",
    "        return self.discussion_scraper.load_from_json()\n",
    "    elif section == \"model\":\n",
    "        return self.model_scraper.load_from_json()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def structure_deep_scraped_results(self, deep_scraped_results, section):\n",
    "    structured_results = []\n",
    "\n",
    "    for result in deep_scraped_results:\n",
    "        content_hash = compute_content_hash(result.get(\"content\", \"\"))\n",
    "        cached_data = self.get_cache(content_hash)\n",
    "\n",
    "        if cached_data and cached_data[\"deep_scraped\"]:\n",
    "            logger.info(f\"Skipping already structured item: {result.get('title', '')}\")\n",
    "            continue\n",
    "\n",
    "        if section == \"code\":\n",
    "            structured = self.notebook_scraper.parse({\n",
    "                \"title\": result.get(\"title\", \"\"),\n",
    "                \"url\": result.get(\"url\", \"\"),\n",
    "                \"section\": section,\n",
    "                \"relevance_score\": result.get(\"relevance_score\", None),\n",
    "                \"metadata\": result.get(\"metadata\", {}),\n",
    "                \"content\": result.get(\"markdown_blocks\", \"\")\n",
    "            })\n",
    "        elif section == \"discussion\":\n",
    "            structured = self.discussion_scraper.parse({\n",
    "                \"title\": result.get(\"title\", \"\"),\n",
    "                \"url\": result.get(\"url\", \"\"),\n",
    "                \"section\": section,\n",
    "                \"relevance_score\": result.get(\"relevance_score\", None),\n",
    "                \"metadata\": result.get(\"metadata\", {}),\n",
    "                \"content\": result.get(\"ocr_content\", \"\")\n",
    "            })\n",
    "        elif section == \"model\":\n",
    "            structured = self.model_scraper.parse({\n",
    "                \"title\": result.get(\"title\", \"\"),\n",
    "                \"url\": result.get(\"url\", \"\"),\n",
    "                \"section\": section,\n",
    "                \"relevance_score\": result.get(\"relevance_score\", None),\n",
    "                \"metadata\": result.get(\"metadata\", {}),\n",
    "                \"content\": result.get(\"model_card_details\", \"\")\n",
    "            })\n",
    "        else:\n",
    "            logger.warning(f\"No parser for section: {section}\")\n",
    "            continue\n",
    "\n",
    "        structured_dict = structured.dict()\n",
    "        structured_dict.update({\n",
    "            \"deep_scraped\": True,\n",
    "            \"deep_scraped_at\": datetime.now(datetime.timezone.utc).isoformat(),\n",
    "            \"content_hash\": content_hash\n",
    "        })\n",
    "\n",
    "        structured_results.append(structured_dict)\n",
    "        self.update_cache(content_hash, deep_scraped=True)\n",
    "\n",
    "    return structured_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HayStack RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecefad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install farm-haystack[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d957d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haystack imports\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import PreProcessor\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack import Document\n",
    "from haystack.nodes import SentenceTransformersRanker, TransformersQueryClassifier\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3077b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaystackRAGPipeline():\n",
    "    def __init__(self, index_name=\"kaggle_index\", use_faiss = True, embedding_model = \"BAAI/bge-base-en\"):\n",
    "        \"\"\"\n",
    "        Initializes the RAG pipeline with a FAISS document store and an embedding retriever.\n",
    "        \"\"\"\n",
    "        # 1. Document store setup\n",
    "        self.document_store = FAISSDocumentStore(\n",
    "            faiss_index_factory_str=\"IVF256,Flat\",  # allows future clustering\n",
    "            similarity=\"cosine\",  # or 'dot_product' if using dot-based models\n",
    "            return_embedding=True\n",
    "        )\n",
    "\n",
    "        # 2. Embedding retriever\n",
    "        self.retriever = EmbeddingRetriever(\n",
    "            document_store=self.document_store,\n",
    "            embedding_model=embedding_model,\n",
    "            model_format=\"sentence_transformers\",\n",
    "            use_gpu=True\n",
    "        )\n",
    "\n",
    "        # 3. Optional: Store model separately for direct calls if needed\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "\n",
    "        self.reranker = SentenceTransformersRanker(\n",
    "        model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        use_gpu=True\n",
    ")\n",
    "\n",
    "        # 4. Metadata tracking (optional)\n",
    "        self.indexed_hashes = set()\n",
    "\n",
    "    \n",
    "\n",
    "def index_scraped_data(self, pydantic_results, structured_results): \n",
    "    documents_to_index = []\n",
    "\n",
    "    # Helper function to build Documents\n",
    "    def create_document(item, source_type):\n",
    "        content = item.get(\"content\", \"\")\n",
    "        metadata = item.get(\"metadata\", {})\n",
    "        metadata.update({\n",
    "            \"content_hash\": item.get(\"content_hash\"),\n",
    "            \"source\": source_type,\n",
    "            \"deep_scraped\": item.get(\"deep_scraped\", False),\n",
    "        })\n",
    "        return Document(content=content, meta=metadata)\n",
    "\n",
    "    for result in pydantic_results:\n",
    "        documents_to_index.append(create_document(result, \"scraped\"))\n",
    "\n",
    "    for result in structured_results:\n",
    "        documents_to_index.append(create_document(result, \"deep_scraped\"))\n",
    "\n",
    "    # Write to vector store\n",
    "    self.document_store.write_documents(documents_to_index)\n",
    "\n",
    "    # Compute embeddings only for scraped and deep_scraped data\n",
    "    self.document_store.update_embeddings(\n",
    "        self.retriever,\n",
    "        filters={\"source\": [\"scraped\", \"deep_scraped\"]}\n",
    "    )\n",
    "\n",
    "\n",
    "def index_api_data(self, api_results):\n",
    "    documents_to_index = []\n",
    "    for api_data in api_results:\n",
    "        if not api_content.strip():\n",
    "            continue  # Skip empty documents\n",
    "        api_content = api_data.get(\"content\", \"\")\n",
    "        embed_api_content = self.embedding_model.encode(api_content)\n",
    "\n",
    "        documents_to_index.append({\n",
    "            \"content\" : api_content,\n",
    "            \"embedding\": embed_api_content\n",
    "            })\n",
    "    self.document_store.write_documents(documents_to_index)\n",
    "\n",
    "def chunk_and_index(self, pydantic_results, structured_results):\n",
    "    # define documents to be chunked\n",
    "    all_results = pydantic_results + structured_results\n",
    "    overview_and_discussion_docs = [result for result in all_results if result.get(\"section\") in {\"overview\", \"discussion\"}]\n",
    "    # Use Preprocesser to do chunking\n",
    "    \n",
    "    preprocessor = PreProcessor(\n",
    "        split_by=\"sentence\",           # or \"word\", \"passage\"\n",
    "        split_length=100,              # adjust chunk size\n",
    "        split_overlap=20,              # for context preservation\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        remove_substrings=None\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in overview_and_discussion_docs:\n",
    "        preprocessed = preprocessor.process(documents =[{\"content\": doc[\"content\"], \"meta\": doc}])\n",
    "        chunks.extend(preprocessed)\n",
    "    \n",
    "    self.document_store.write_documents(chunks)\n",
    "    self.document_store.update_embeddings(self.retriever)\n",
    "\n",
    "    return f\"Successfully chunked and indexed {len(chunks)} documents.\"\n",
    "        \n",
    "\n",
    "    \n",
    "def rerank_document_store(self, query: str, top_k_retrieval: int = 20, top_k_final: int = 5):\n",
    "    retrieved_docs = self.retriever.retrieve(query=query, top_k=top_k_retrieval)\n",
    "    reranked_docs = self.reranker.predict(query=query, documents=retrieved_docs)\n",
    "    return reranked_docs[:top_k_final]\n",
    "\n",
    "\n",
    "\n",
    "def log_retrieval(self, query, retrieved_docs, section=None):\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    logger.info(f\"[{timestamp}] Retrieval Log:\")\n",
    "    logger.info(f\"🔍 Query: {query}\")\n",
    "    if section:\n",
    "        logger.info(f\"📂 Section: {section}\")\n",
    "    logger.info(f\"📄 Retrieved {len(retrieved_docs)} documents.\")\n",
    "\n",
    "    for i, doc in enumerate(retrieved_docs[:5]):  # Log first few for brevity\n",
    "        logger.info(f\"  📘 Document {i + 1}:\")\n",
    "        logger.info(f\"    Metadata: {doc.meta}\")\n",
    "        snippet = doc.content[:200].replace('\\n', ' ') + \"...\"  # Preview\n",
    "        logger.info(f\"    Content Snippet: {snippet}\")\n",
    "   \n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed39888",
   "metadata": {},
   "source": [
    "For creating Haystack document store possible improvements\n",
    "\n",
    "Later you could merge results from both stores for hybrid querying\n",
    "\n",
    "Or apply different reranking strategies depending on the source\n",
    "\n",
    "Tag clusters based on section, source, topic, etc., for better relevance filtering\n",
    "\n",
    "Consider creating two different stores and merging results if results are not satisfactory.\n",
    "\n",
    "\n",
    "Chunking logic can be modified to improve retrieval results\n",
    "\n",
    "\n",
    "\n",
    "Can use different models for reranking if the current one is not working out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kaggle Expert Agentic system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f893d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing user input\n",
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate  # optional\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import re\n",
    "import os\n",
    "import importlib.util\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from spellchecker import spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a794db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInputProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the UserInputProcessor which analyzes, cleans, and classifies\n",
    "        the incoming user query before handing it off to the orchestrator.\n",
    "        \"\"\"\n",
    "        # You can add regex patterns, keyword templates, or few-shot examples here later\n",
    "        self.session_state = {}  # e.g., to track username or competition name if mentioned\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast\n",
    "        self.intent_map = {\n",
    "            \"onboarding\": [\n",
    "                \"just joined this competition\",\n",
    "                \"first time\",\n",
    "                \"beginner\",\n",
    "                \"getting started\"\n",
    "            ],\n",
    "            \"progress\": [\n",
    "                \"started exploring\",\n",
    "                \"here is my code\",\n",
    "                \"i tried\",\n",
    "                \"my current approach\"\n",
    "            ],\n",
    "            \"confusion\": [\n",
    "                \"not sure\",\n",
    "                \"don’t understand\",\n",
    "                \"explain this code\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.intent_embeddings = {\n",
    "        intent: self.model.encode(phrases, convert_to_tensor=True)\n",
    "        for intent, phrases in self.intent_map.items()\n",
    "        }\n",
    "        self.threshold = 0.65\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_kaggle_query(self, query, remove_stopwords=False, spellcheck=False, detect_language=False, **kwargs):\n",
    "       \"\"\"\n",
    "    Enhanced preprocessing for Kaggle queries.\n",
    "    Adds spellchecking, lemmatization, language detection, and richer metadata.\n",
    "    \"\"\"\n",
    "             # Lowercase\n",
    "       tokens = improved_preprocess(query, remove_stopwords=remove_stopwords)\n",
    "       \n",
    "       if spellcheck:\n",
    "           try:\n",
    "               spell = SpellChecker()\n",
    "               tokens = [spell.correction(word) for word in tokens]\n",
    "           except Exception:\n",
    "               pass\n",
    "           \n",
    "       if remove_stopwords:\n",
    "           stop_words = set(stopwords.words('english'))\n",
    "           tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Optional metadata\n",
    "       contains_code = bool(re.search(r'```|import |def |class |\\=', query))\n",
    "       contains_url = bool(re.search(r'http[s]?://', query))\n",
    "       contains_number = any(char.isdigit() for char in query)\n",
    "       contains_question = \"?\" in query\n",
    "       length = len(tokens)\n",
    "        \n",
    "       return {\n",
    "        \"cleaned_query\": \" \".join(tokens),\n",
    "        \"original_query\": query,\n",
    "        \"contains_code\": contains_code,\n",
    "        \"contains_url\": contains_url,\n",
    "        \"contains_number\": contains_number,\n",
    "        \"contains_question\": contains_question,\n",
    "        \"tokens\": tokens,\n",
    "        \"length\": length,\n",
    "}\n",
    "\n",
    "    def classify_with_embeddings(self, cleaned_input: str):\n",
    "        input_embedding = self.model.encode(cleaned_input, convert_to_tensor=True)\n",
    "        best_intent = None\n",
    "        best_score = -1\n",
    "\n",
    "        for intent, pattern_embeddings in self.intent_embeddings.items():\n",
    "            scores = util.pytorch_cos_sim(input_embedding, pattern_embeddings)\n",
    "            max_score = scores.max().item()\n",
    "\n",
    "            if max_score > best_score:\n",
    "                best_score = max_score\n",
    "                best_intent = intent\n",
    "\n",
    "        return best_intent, best_score\n",
    "\n",
    "    def classify_with_llm(self, cleaned_input: str):\n",
    "        categories = list(self.intent_map.keys())\n",
    "        prompt = f\"\"\"\n",
    "You're a helpful assistant categorizing user queries on a machine learning competition forum.\n",
    "\n",
    "Given the following categories:\n",
    "- onboarding: for users who are new or just joined\n",
    "- progress: for users sharing current work or exploration\n",
    "- confusion: for users asking for help or expressing confusion\n",
    "\n",
    "Classify this query into one of the above categories:\n",
    "\"{cleaned_input}\"\n",
    "\n",
    "Respond with only the category name (onboarding, progress, confusion).\n",
    "\"\"\"\n",
    "        response = self.llm.predict(prompt).strip().lower()\n",
    "        return response if response in categories else \"other\"\n",
    "\n",
    "    def classify_intent(self, cleaned_input: str):\n",
    "        embedding_intent, score = self.classify_with_embeddings(cleaned_input)\n",
    "\n",
    "        if score >= self.threshold:\n",
    "            return embedding_intent\n",
    "        else:\n",
    "            return self.classify_with_llm(cleaned_input)\n",
    "        \n",
    "    def structure_query(self, original_query):\n",
    "        cleaned_info = self.preprocess_query(original_query)\n",
    "        intent = self.classify_intent(cleaned_info[\"cleaned_query\"])\n",
    "\n",
    "        return {\n",
    "            \"original_query\": original_query,\n",
    "            \"cleaned_query\": cleaned_info[\"cleaned_query\"],\n",
    "            \"intent\": intent,\n",
    "            \"metadata\": {\n",
    "            \"contains_code\": cleaned_info[\"contains_code\"],\n",
    "            \"contains_url\": cleaned_info[\"contains_url\"]\n",
    "        },\n",
    "        \"tokens\": cleaned_info.get(\"tokens\", []),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee3e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Core Python\n",
    "from tabnanny import verbose\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "from importlib import metadata\n",
    "import json\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chains.router import RouterChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "# Our agent handlers\n",
    "from null.competition_summary import CompetitionSummaryAgent\n",
    "from null.timeline_coach import TimelineCoachAgent\n",
    "from null.notebook_explainer import NotebookExplainerAgent\n",
    "from null.code_feedback import CodeFeedbackAgent\n",
    "from null.multi_hop_reasoning import MultiHopReasoningAgent\n",
    "from null.error_diagnosis import ErrorDiagnosisAgent\n",
    "from null.discussion_helper import DiscussionHelperAgent\n",
    "from null.progress_monitor import ProgressMonitorAgent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExpertSystemOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.routing_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "\n",
    "        # === Step 1: Define agent descriptions ===\n",
    "        self.destinations = {\n",
    "            \"competition_summary\": {\n",
    "                \"name\": \"competition_summary\",\n",
    "                \"description\": \"Handles general questions about the competition goal, rules, or overview.\",\n",
    "                \"chain\": CompetitionSummaryAgent()\n",
    "            },\n",
    "            \"timeline_coach\": {\n",
    "                \"name\": \"timeline_coach\",\n",
    "                \"description\": \"Helps the user plan realistic timelines and project checkpoints.\",\n",
    "                \"chain\": TimelineCoachAgent()\n",
    "            },\n",
    "            \"notebook_explainer\": {\n",
    "                \"name\": \"notebook_explainer\",\n",
    "                \"description\": \"Explains code snippets and pinned notebook logic to beginners.\",\n",
    "                \"chain\": NotebookExplainerAgent()\n",
    "            },\n",
    "            \"code_feedback\": {\n",
    "                \"name\": \"code_feedback\",\n",
    "                \"description\": \"Reviews user code and offers suggestions for improvement.\",\n",
    "                \"chain\": CodeFeedbackAgent()\n",
    "            },\n",
    "            \"multi_hop_reasoning\": {\n",
    "                \"name\": \"multi_hop_reasoning\",\n",
    "                \"description\": \"Answers questions requiring multiple sources or steps to resolve.\",\n",
    "                \"chain\": MultiHopReasoningAgent()\n",
    "            },\n",
    "            \"error_diagnosis\": {\n",
    "                \"name\": \"error_diagnosis\",\n",
    "                \"description\": \"Helps identify and fix errors or unexpected behaviors in code.\",\n",
    "                \"chain\": ErrorDiagnosisAgent()\n",
    "            },\n",
    "            \"discussion_helper\": {\n",
    "                \"name\": \"discussion_helper\",\n",
    "                \"description\": \"Formulates meaningful questions or clarifies confusing forum replies.\",\n",
    "                \"chain\": DiscussionHelperAgent()\n",
    "            },\n",
    "            \"progress_monitor\": {\n",
    "                \"name\": \"progress_monitor\",\n",
    "                \"description\": \"Analyzes the user’s past progress and advises next steps.\",\n",
    "                \"chain\": ProgressMonitorAgent()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # === Step 2: Build destination chains ===\n",
    "        self.agent_chains = {\n",
    "            key: value[\"chain\"] for key, value in self.destinations.items()\n",
    "        }\n",
    "\n",
    "        self.llm = ChatOpenAI(temperature=0.3,\n",
    "    model_name=\"gpt-4\",  # or gpt-4o, gpt-3.5-turbo, etc.\n",
    ")       \n",
    "        self.aggregation_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a smart AI orchestrator. Several expert agents have responded to a user's question.\n",
    "\n",
    "User Intent: {intent}\n",
    "Sub-intents: {sub_intents}\n",
    "Reasoning Style: {reasoning_style}\n",
    "\n",
    "Here are their responses:\n",
    "{agent_responses}\n",
    "\n",
    "Based on these, synthesize a coherent and helpful answer. Make sure to:\n",
    "- Combine overlapping insights\n",
    "- Prioritize concrete suggestions\n",
    "- Align with the user's intent and reasoning needs\n",
    "- Omit redundant explanations\n",
    "- Be concise, clear, and actionable\n",
    "\"\"\")\n",
    "\n",
    "        # Aggregation LLM chain\n",
    "        \n",
    "        self.aggregation_chain = LLMChain(prompt=self.aggregation_prompt, llm=self.llm, verbose=True)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def route_to_agents(self, original_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Uses few-shot prompting to classify the query and generate a structured query dict.\n",
    "        Returns a structured query for downstream use.\n",
    "        \"\"\"\n",
    "        few_shot_prompt = f\"\"\"\n",
    "You are an expert query router. Given a user query, extract:\n",
    "\n",
    "- intent: main goal (e.g., code_feedback, timeline_coach, error_diagnosis, discussion_helper, multi_hop_reasoning, conversational)\n",
    "- sub_intents: list of secondary goals if any\n",
    "- input_references: any code, notebook, or data references\n",
    "- reasoning_style: e.g., step_by_step, summary, critique, etc.\n",
    "- preferred_agents: list of agent names if user hints at a preference\n",
    "- metadata_flags: any special flags (e.g., urgent, beginner, etc.)\n",
    "- confidence_score: float from 0.0 to 1.0 estimating your certainty\n",
    "- uncertain_parse: true if you’re not confident, else false\n",
    "- query_mode: what type of task is this? (exploration, debugging, synthesis, planning, etc.)\n",
    "\n",
    "Examples:\n",
    "Q: \"Can you review my code and suggest improvements?\"\n",
    "A: {{\n",
    "  \"intent\": \"code_feedback\",\n",
    "  \"sub_intents\": [],\n",
    "  \"input_references\": [],\n",
    "  \"reasoning_style\": \"critique\",\n",
    "  \"preferred_agents\": [\"code_feedback\"],\n",
    "  \"metadata_flags\": {{}},\n",
    "  \"confidence_score\": 0.95,\n",
    "  \"uncertain_parse\": false,\n",
    "  \"query_mode\": \"debugging\"\n",
    "}}\n",
    "\n",
    "Q: \"I'm stuck on the timeline, what should I do next?\"\n",
    "A: {{\n",
    "  \"intent\": \"timeline_coach\",\n",
    "  \"sub_intents\": [],\n",
    "  \"input_references\": [],\n",
    "  \"reasoning_style\": \"step_by_step\",\n",
    "  \"preferred_agents\": [\"timeline_coach\"],\n",
    "  \"metadata_flags\": {{}},\n",
    "  \"confidence_score\": 0.92,\n",
    "  \"uncertain_parse\": false,\n",
    "  \"query_mode\": \"planning\"\n",
    "}}\n",
    "\n",
    "Q: \"Why is my notebook throwing an error?\"\n",
    "A: {{\n",
    "  \"intent\": \"error_diagnosis\",\n",
    "  \"sub_intents\": [],\n",
    "  \"input_references\": [\"notebook\"],\n",
    "  \"reasoning_style\": \"step_by_step\",\n",
    "  \"preferred_agents\": [\"error_diagnosis\"],\n",
    "  \"metadata_flags\": {{}},\n",
    "  \"confidence_score\": 0.85,\n",
    "  \"uncertain_parse\": false,\n",
    "  \"query_mode\": \"debugging\"\n",
    "}}\n",
    "\n",
    "Q: \"{original_query}\"\n",
    "A:\n",
    "\"\"\".format(query=original_query)\n",
    "\n",
    "        # Call your LLM (replace with your actual LLM call)\n",
    "        router_output = self.routing_llm(few_shot_prompt)\n",
    "\n",
    "        # Parse the LLM output as JSON\n",
    "        try:\n",
    "            structured_query = json.loads(router_output)\n",
    "        except Exception as e:\n",
    "            # Fallback: minimal structure\n",
    "            structured_query = {\n",
    "    \"intent\": \"reasoning\",\n",
    "    \"sub_intents\": [],\n",
    "    \"input_references\": [],\n",
    "    \"reasoning_style\": \"default\",\n",
    "    \"preferred_agents\": [],\n",
    "    \"metadata_flags\": {},\n",
    "    \"confidence_score\": 0.0,\n",
    "    \"uncertain_parse\": True,\n",
    "    \"query_mode\": \"exploration\",\n",
    "    \"llm_parse_error\": str(e),\n",
    "    \"raw_output\": router_output\n",
    "                \n",
    "            }\n",
    "\n",
    "        # Add the destination chain name for downstream use\n",
    "        intent = structured_query.get(\"intent\", \"reasoning\")\n",
    "        structured_query[\"destination_chain_name\"] = intent if intent in self.agent_chains else None\n",
    "        return structured_query\n",
    "\n",
    "    def aggregate_response(self, agent_responses: List[Dict[str, Any]], context: Optional[Dict[str, Any]] = None) -> str:\n",
    "        responses_text = \"\\n\\n\".join(\n",
    "        [f\"{i+1}. {resp['agent_name']}: {resp['response']}\" for i, resp in enumerate(agent_responses)]\n",
    "    )\n",
    "\n",
    "        intent = context.get(\"intent\", \"general\") if context else \"general\"\n",
    "        sub_intents = \", \".join(context.get(\"sub_intents\", [])) if context else \"none\"\n",
    "        reasoning_style = context.get(\"reasoning_style\", \"default\") if context else \"default\"\n",
    "\n",
    "        return self.aggregation_chain.run(\n",
    "        intent=intent,\n",
    "        sub_intents=sub_intents,\n",
    "        reasoning_style=reasoning_style,\n",
    "        agent_responses=responses_text\n",
    "    )\n",
    "    def handle_query(self, structured_query: Dict[str, Any], original_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Orchestrator's main method: route -> delegate -> aggregate -> respond.\n",
    "        \"\"\"\n",
    "        # Step 1: Route the query\n",
    "        route_result = self.route_to_agents(original_query)\n",
    "        destination_chain_name = route_result.get(\"destination_chain_name\")\n",
    "\n",
    "        if not destination_chain_name:\n",
    "            return route_result.get(\"error\", \"Routing failed — no destination agent selected.\")\n",
    "\n",
    "        # Step 2: Run the selected agent(s)\n",
    "        if isinstance(destination_chain_name, list):\n",
    "            agent_responses = [\n",
    "                self.agent_chains[name].run(structured_query) for name in destination_chain_name\n",
    "            ]\n",
    "        else:\n",
    "            agent_responses = [self.agent_chains[destination_chain_name].run(structured_query)]\n",
    "\n",
    "        # Wrap responses in dicts for aggregation context (name, response)\n",
    "        formatted_responses = [\n",
    "            {\"agent_name\": name, \"response\": resp}\n",
    "            for name, resp in zip(\n",
    "                destination_chain_name if isinstance(destination_chain_name, list) else [destination_chain_name],\n",
    "                agent_responses\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Step 3: Aggregate and return\n",
    "        return self.aggregate_response(formatted_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0b5a9",
   "metadata": {},
   "source": [
    "Things to consider for later (aggregate_response method)\n",
    "Numbering or agent names in the summary for context\n",
    "\n",
    "Future: Add LLM fallback logic (e.g., if aggregation fails, return top 2 responses)\n",
    "\n",
    "Logging/debug output before LLM aggregation for transparency\n",
    "\n",
    "\n",
    "Things to consider for later (handle_query method)\n",
    "You could cache agent outputs later for optimization\n",
    "\n",
    "Add fallback agents if one fails to respond\n",
    "\n",
    "Log the chain name → response mapping for transparency/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.language_models.chat_models import BaseChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from typing import Dict, Any\n",
    "class CompetitionSummaryAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        competition_context: Dict[str, Any],\n",
    "        haystack_rag_pipeline=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the agent with access to the LLM, static competition metadata,\n",
    "        and optionally a Haystack RAG pipeline for dynamic semantic retrieval.\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.competition_context = competition_context\n",
    "        self.haystack_rag_pipeline = haystack_rag_pipeline or HaystackRAGPipeline()\n",
    "        self.name = \"CompetitionSummaryAgent\"\n",
    "\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\", \"overview\", \"data\", \"evaluation\", \"timeline\"],\n",
    "            template=\"\"\"\n",
    "You're an expert Kaggle competition assistant. The user asked:\n",
    "\"{query}\"\n",
    "\n",
    "Based on the competition details below, generate a helpful summary that aligns with their intent.\n",
    "\n",
    "--- OVERVIEW ---\n",
    "{overview}\n",
    "\n",
    "--- DATA ---\n",
    "{data}\n",
    "\n",
    "--- EVALUATION ---\n",
    "{evaluation}\n",
    "\n",
    "--- TIMELINE ---\n",
    "{timeline}\n",
    "\n",
    "Return a concise yet informative response tailored to the query.\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
    "\n",
    "    def fetch_data_sections(self, query: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Use HaystackRAGPipeline to retrieve relevant data/document sections for the query.\n",
    "        \"\"\"\n",
    "        # Retrieve only documents from the 'data' section\n",
    "        docs = self.haystack_rag_pipeline.retriever.retrieve(\n",
    "            query=query,\n",
    "            top_k=20,\n",
    "            filters={\"section\": \"data\"}\n",
    "        )\n",
    "        reranked = self.haystack_rag_pipeline.reranker.predict(query=query, documents=docs)\n",
    "        # Join top_k document contents\n",
    "        return \"\\n\\n\".join(doc.content for doc in reranked[:top_k])\n",
    "\n",
    "\n",
    "    def run(self, structured_query: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Takes a structured query and generates a competition summary.\n",
    "        Uses HaystackRAGPipeline to enrich the data section if the query references data.\n",
    "        \"\"\"\n",
    "        query = structured_query.get(\"cleaned_query\", \"\")\n",
    "        metadata = structured_query.get(\"metadata\", {})\n",
    "        needs_data = \"data\" in query.lower() or metadata.get(\"needs_data_section\", False)\n",
    "\n",
    "        # Use static metadata unless dynamic retrieval is needed\n",
    "        context = {\n",
    "            \"overview\": self.competition_context.get(\"overview\", \"\"),\n",
    "            \"data\": self.competition_context.get(\"data\", \"\"),\n",
    "            \"evaluation\": self.competition_context.get(\"evaluation\", \"\"),\n",
    "            \"timeline\": self.competition_context.get(\"timeline\", \"\")\n",
    "        }\n",
    "\n",
    "        # If query references data, use HaystackRAGPipeline to retrieve relevant data sections\n",
    "        if needs_data and self.haystack_rag_pipeline:\n",
    "            try:\n",
    "                rag_data = self.fetch_data_sections(query)\n",
    "                if rag_data:\n",
    "                    context[\"data\"] = rag_data\n",
    "            except Exception as e:\n",
    "                context[\"data\"] += f\"\\n[Note: Error retrieving data section from RAG: {str(e)}]\"\n",
    "        # Final chain input\n",
    "        inputs = {\n",
    "            \"query\": query,\n",
    "            **context\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.chain.run(inputs)\n",
    "        except Exception as e:\n",
    "            response = f\"[Error in {self.name}] {str(e)}\"\n",
    "\n",
    "        return {\n",
    "            \"agent_name\": self.name,\n",
    "            \"response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from crewai import Agent as CrewAIAgent, Tool as CrewTool\n",
    "from autogen import AssistantAgent as AutoGenAssistant\n",
    "from autogen.agentchat.memory_utils import SimpleMemory\n",
    "class TimelineCoachAgent:\n",
    "    def __init__(self, llm: BaseLanguageModel):\n",
    "        self.name = \"TimelineCoachAgent\"\n",
    "        self.memory = SimpleMemory()\n",
    "        self.llm = llm\n",
    "\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            input_key=\"progress_update\",\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"timeline\", \"progress_update\", \"current_date\", \"chat_history\"],\n",
    "            template=\"\"\"\n",
    "You are a helpful Kaggle mentor that provides progress-aware guidance to users during competitions.\n",
    "\n",
    "Competition Timeline:\n",
    "{timeline}\n",
    "\n",
    "User's Current Progress:\n",
    "{progress_update}\n",
    "\n",
    "Prior Progress & Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Today’s Date: {current_date}\n",
    "\n",
    "Advise the user on what to prioritize next.\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt_template,\n",
    "            memory=self.memory\n",
    "        )\n",
    "\n",
    "    def run(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
    "        inputs.setdefault(\"chat_history\", self.memory.buffer_as_str())  # fallback if not passed explicitly\n",
    "        result = self.chain.run(inputs)\n",
    "        return {\n",
    "            \"agent_name\": self.name,\n",
    "            \"response\": result\n",
    "        }\n",
    "\n",
    "    def get_crew_agent(self):\n",
    "        def timeline_tool(timeline: str, progress_update: str, current_date: str = None) -> str:\n",
    "            if not current_date:\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            inputs = {\n",
    "                \"timeline\": timeline,\n",
    "                \"progress_update\": progress_update,\n",
    "                \"current_date\": current_date\n",
    "            }\n",
    "            result = self.run(inputs)\n",
    "            return result.get(\"response\", \"[No response generated]\")\n",
    "\n",
    "        tool = CrewTool(\n",
    "            name=\"Timeline Coaching Tool\",\n",
    "            description=\"Analyzes user progress in relation to a competition timeline and suggests what to prioritize next.\",\n",
    "            func=timeline_tool\n",
    "        )\n",
    "\n",
    "        return CrewAIAgent(\n",
    "            role=\"Kaggle Timeline Coach\",\n",
    "            goal=\"Guide users throughout the competition timeline based on their current progress and date.\",\n",
    "            backstory=\"You are a supportive mentor with memory of user progress and timeline context. Your job is to keep users focused and aligned with deadlines.\",\n",
    "            tools=[tool],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def get_autogen_agent(self, llm_config: Optional[dict] = None) -> AutoGenAssistant:\n",
    "        if llm_config is None:\n",
    "            llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "            \"model\": \"llama3\",\n",
    "            \"api_base\": \"http://localhost:11434/v1\",\n",
    "            \"api_key\": \"ollama\", \n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "\n",
    "        return AutoGenAssistant(\n",
    "            name=self.name,\n",
    "            llm_config=llm_config,\n",
    "            memory=self.memory,\n",
    "            system_message=(\n",
    "                \"You are a timeline-aware Kaggle competition mentor AI. \"\n",
    "                \"Given a user's progress and a timeline, suggest next steps to help them stay on track. \"\n",
    "                \"You remember previous guidance given to the user to maintain consistency.\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfd78e",
   "metadata": {},
   "source": [
    "Maybe for later\n",
    "\n",
    "A reset method for the memory (reset_memory()).\n",
    "\n",
    "A way to export chat_history for display in your app logs or UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "from langchain.chat_models import ChatOpenAI  # or a free model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd970530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookExplainerAgent:\n",
    "    def __init__(self, retriever=None, llm=None):\n",
    "        \"\"\"\n",
    "        Initialize the NotebookExplainerAgent.\n",
    "\n",
    "        Args:\n",
    "            retriever: An instance of HaystackRAGPipeline or similar.\n",
    "            llm: An optional language model for explanation.\n",
    "        \"\"\"\n",
    "        self.retriever = retriever or HaystackRAGPipeline()\n",
    "        self.llm = llm\n",
    "        self.chain = self._build_explanation_chain()\n",
    "        self.name = \"NotebookExplainerAgent\"\n",
    "\n",
    "    def _build_explanation_chain(self) -> LLMChain:\n",
    "        template = PromptTemplate.from_template(\n",
    "            \"\"\"You are an expert Kaggle competition assistant. Explain the following notebook or model section to a beginner user.\n",
    "\n",
    "Section:\n",
    "-----------------\n",
    "{section_content}\n",
    "\n",
    "Explanation (use friendly language, include purpose and any ML context):\n",
    "\"\"\"\n",
    "        )\n",
    "        return LLMChain(llm=self.llm, prompt=template)\n",
    "\n",
    "    def fetch_sections(self, query: Dict[str, Any], section: str = \"code\", top_k: int = 5) -> list:\n",
    "        \"\"\"\n",
    "        Fetch notebook or model content using the retriever.\n",
    "        Returns a list of relevant chunks.\n",
    "        \"\"\"\n",
    "        cleaned_query = query.get(\"cleaned_query\", \"\")\n",
    "        docs = self.retriever.retriever.retrieve(\n",
    "            query=cleaned_query,\n",
    "            top_k=20,\n",
    "            filters={\"section\": section}\n",
    "        )\n",
    "        reranked = self.retriever.reranker.predict(query=cleaned_query, documents=docs)\n",
    "        return reranked[:top_k]\n",
    "    \n",
    "\n",
    "\n",
    "    def extract_relevant_sections(self, chunks: list, query: Dict[str, Any]) -> list:\n",
    "        \"\"\"\n",
    "        Extract only the parts relevant to the user's question.\n",
    "        \"\"\"\n",
    "        question = query.get(\"cleaned_query\", \"\")\n",
    "        relevant = []\n",
    "        for chunk in chunks:\n",
    "            if any(word in chunk.content.lower() for word in question.split()):\n",
    "                relevant.append(chunk.content)\n",
    "        return relevant[:5]\n",
    "    def explain_sections(self, sections: list) -> str:\n",
    "        \"\"\"\n",
    "        Use the LLM chain to explain each section, then concatenate the results.\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        for section_content in sections:\n",
    "            explanation = self.chain.run({\"section_content\": section_content})\n",
    "            explanations.append(explanation)\n",
    "        return \"\\n\\n\".join(explanations)\n",
    "\n",
    "    def run(self, structured_query: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = structured_query.get(\"query\", \"\")\n",
    "        metadata = structured_query.get(\"metadata\", {})\n",
    "        user_level = metadata.get(\"user_level\", \"beginner\")\n",
    "        tone = metadata.get(\"tone\", \"friendly\")\n",
    "        section = \"code\"\n",
    "\n",
    "        # If the query references models, switch to model section retrieval\n",
    "        if \"model\" in query.lower() or metadata.get(\"needs_model_section\", False):\n",
    "            section = \"model\"\n",
    "\n",
    "        try:\n",
    "            chunks = self.fetch_sections(structured_query, section=section, top_k=5)\n",
    "            relevant_chunks = self.extract_relevant_sections(chunks, structured_query)\n",
    "            explanations = []\n",
    "            for section_content in relevant_chunks:\n",
    "                prompt_input = {\n",
    "                    \"section_content\": section_content,\n",
    "                    \"user_level\": user_level,\n",
    "                    \"tone\": tone\n",
    "                }\n",
    "                explanation = self.chain.run(prompt_input)\n",
    "                explanations.append(explanation)\n",
    "            final_response = \"\\n\\n\".join(explanations)\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": final_response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": f\"Notebook/model explanation failed: {str(e)}\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c572b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759870b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from crewai import Agent as CrewAIAgent, Tool as CrewTool\n",
    "from autogen import AssistantAgent as AutoGenAssistant\n",
    "from autogen.agentchat.memory_utils import SimpleMemory\n",
    "class CodeFeedbackAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"CodeFeedbackAgent\"\n",
    "        self.memory = SimpleMemory()\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "You are a seasoned Kaggle code reviewer AI.\n",
    "\n",
    "The user has shared a snippet of code. Your job is to:\n",
    "1. Analyze the code for clarity, style, and efficiency.\n",
    "2. Offer suggestions on improving readability and performance.\n",
    "3. Identify any logic or structural problems.\n",
    "4. Do NOT rewrite the entire code unless it’s very short — provide focused suggestions instead.\n",
    "5. Use bullet points for feedback when appropriate.\n",
    "\n",
    "Code Snippet:\n",
    "{code}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = LLMChain(\n",
    "            llm=ChatOpenAI(\n",
    "                model_name=\"gpt-4o\",\n",
    "                temperature=0.3\n",
    "            ),\n",
    "            prompt=self.prompt\n",
    "        )\n",
    "\n",
    "    def run(self, code: Union[str, dict]) -> dict:\n",
    "        result = self.chain.run({\"code\": code})\n",
    "        return {\n",
    "            \"agent_name\": self.name,\n",
    "            \"response\": result\n",
    "        }\n",
    "\n",
    "    def get_crew_agent(self):\n",
    "        def feedback_tool(code: str) -> str:\n",
    "            result = self.run(code)\n",
    "            return result.get(\"response\", \"[No feedback generated]\")\n",
    "\n",
    "        tool = CrewTool(\n",
    "            name=\"Code Feedback Tool\",\n",
    "            description=\"Analyzes a user's code snippet and provides feedback to improve it.\",\n",
    "            func=feedback_tool\n",
    "        )\n",
    "\n",
    "        return CrewAIAgent(\n",
    "            role=\"Kaggle Code Reviewer\",\n",
    "            goal=\"Provide constructive code feedback to help users improve their notebooks.\",\n",
    "            backstory=\"You're a skilled AI reviewer for Kaggle code who evaluates clarity, performance, and style, and gives precise improvement tips.\",\n",
    "            tools=[tool],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def get_autogen_agent(self, llm_config: Optional[dict] = None) -> AutoGenAssistant:\n",
    "        if llm_config is None:\n",
    "            llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "            \"model\": \"llama3\",\n",
    "            \"api_base\": \"http://localhost:11434/v1\",\n",
    "            \"api_key\": \"ollama\", \n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "\n",
    "        return AutoGenAssistant(\n",
    "            name=self.name,\n",
    "            llm_config=llm_config,\n",
    "            memory=self.memory,\n",
    "            system_message=(\n",
    "                \"You are a skilled AI code reviewer helping Kaggle users refine their code. \"\n",
    "                \"Offer focused suggestions on improving clarity, structure, and performance. \"\n",
    "                \"Only rewrite snippets when necessary. Respond clearly and helpfully.\"\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Dict, Any\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "\n",
    "class DiscussionHelperAgent:\n",
    "    def __init__(self, llm: BaseChatModel):\n",
    "        self.name = \"DiscussionHelperAgent\"\n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a Kaggle mentor agent helping users communicate effectively in the discussion forums.\n",
    "\n",
    "The user is currently facing this issue:\n",
    "\"{query}\"\n",
    "\n",
    "User level: {user_level}\n",
    "Intent: {intent}\n",
    "Preferred tone: {tone}\n",
    "\n",
    "Generate:\n",
    "1. A high-quality Kaggle discussion post they can make to get help or feedback.\n",
    "2. Optional clarification questions they can include to improve engagement.\n",
    "3. If applicable, suggest improvements to make the post clearer or more targeted.\n",
    "\n",
    "Use the tone specified. Keep it concise but rich in context.\n",
    "\"\"\")\n",
    "\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def run(self, structured_query: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = structured_query.get(\"query\", \"\")\n",
    "        metadata = structured_query.get(\"metadata\", {})\n",
    "        user_level = metadata.get(\"user_level\", \"intermediate\")\n",
    "        tone = metadata.get(\"tone\", \"neutral\")\n",
    "        intent = metadata.get(\"intent\", \"ask for help\")\n",
    "\n",
    "        try:\n",
    "            response = self.chain.run({\n",
    "                \"query\": query,\n",
    "                \"user_level\": user_level,\n",
    "                \"tone\": tone,\n",
    "                \"intent\": intent\n",
    "            })\n",
    "\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": f\"[Error in {self.name}] {str(e)}\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from crewai import Agent as CrewAIAgent, Tool as CrewTool\n",
    "from autogen import AssistantAgent as AutoGenAssistant\n",
    "from autogen.agentchat.memory_utils import SimpleMemory\n",
    "class MultiHopReasoningAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"MultiHopReasoningAgent\"\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "        self.memory = SimpleMemory()\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an expert AI agent trained in multi-hop reasoning for data science competitions.\n",
    "\n",
    "        The user has a question that may require combining information from multiple areas such as:\n",
    "        - The competition overview\n",
    "        - The datasets and their structure\n",
    "        - Code and models in notebooks\n",
    "        - Insights from discussion forums\n",
    "\n",
    "        Question:\n",
    "        {input}\n",
    "\n",
    "        Think through the required steps, extract insights from relevant components, and then answer the question clearly and logically.\n",
    "        \"\"\")\n",
    "\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def run(self, query: str) -> dict:\n",
    "        result = self.chain.run(query)\n",
    "        return {\n",
    "            \"agent_name\": \"multi_hop_reasoning\",\n",
    "            \"response\": result\n",
    "        }\n",
    "\n",
    "    def get_crew_agent(self):\n",
    "        def reasoning_tool(input: str) -> str:\n",
    "            result = self.run(input)\n",
    "            return result.get(\"response\", \"[No response generated]\")\n",
    "\n",
    "        tool = CrewTool(\n",
    "            name=\"Multi-Hop Reasoning Tool\",\n",
    "            description=\"Answers complex questions that require reasoning across multiple competition aspects like overview, datasets, notebooks, and discussions.\",\n",
    "            func=reasoning_tool\n",
    "        )\n",
    "\n",
    "        return CrewAIAgent(\n",
    "            role=\"Multi-Hop Reasoning Specialist\",\n",
    "            goal=\"Provide logically sound answers to complex Kaggle-related questions that span multiple data sources.\",\n",
    "            backstory=\"You're an advanced reasoning AI designed to synthesize insights from different aspects of a data science competition.\",\n",
    "            tools=[tool],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def get_autogen_agent(self, llm_config: Optional[dict] = None) -> AutoGenAssistant:\n",
    "        if llm_config is None:\n",
    "            llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "            \"model\": \"llama3\",\n",
    "            \"api_base\": \"http://localhost:11434/v1\",\n",
    "            \"api_key\": \"ollama\", \n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "\n",
    "        return AutoGenAssistant(\n",
    "            name=self.name,\n",
    "            llm_config=llm_config,\n",
    "            memory=self.memory,\n",
    "            system_message=(\n",
    "                \"You are a multi-hop reasoning expert for data science competitions. \"\n",
    "                \"Answer complex questions that require combining knowledge from competition overview, datasets, code notebooks, and discussions. \"\n",
    "                \"Think through each step logically before giving your answer.\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29911dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from crewai import Agent as CrewAIAgent, Tool as CrewTool\n",
    "from autogen import AssistantAgent as AutoGenAssistant, UserProxyAgent as AutoGenUser\n",
    "from autogen.agentchat.memory_utils import SimpleMemory\n",
    "class ProgressMonitorAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"ProgressMonitorAgent\"\n",
    "        self.memory = SimpleMemory()\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are a competition progress analyst AI.\n",
    "\n",
    "            A user is participating in a Kaggle-like data science competition and has told you their current rank: {user_rank}.\n",
    "\n",
    "            Your job is to:\n",
    "            1. Classify the user’s rank into a bracket (e.g., Top 10, Top 100, Top 500, etc.).\n",
    "            2. Provide insights on what users in higher ranks (above them) tend to do.\n",
    "            3. Suggest general strategies to help them move closer to the next tier.\n",
    "            4. Avoid giving specific answers from other notebooks.\n",
    "            5. If possible, suggest collaboration with a Notebook Explainer AI for deeper code-level comparisons (optional for now).\n",
    "\n",
    "            Respond concisely but informatively, in a motivational and analytical tone.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = LLMChain(\n",
    "            llm=ChatOpenAI(model_name=\"gpt-4o\", temperature=0.3),\n",
    "            prompt=self.prompt\n",
    "        )\n",
    "\n",
    "    def run(self, user_rank: Union[str, int]) -> str:\n",
    "        return self.chain.run({\"user_rank\": str(user_rank)})\n",
    "\n",
    "    def get_crew_agent(self):\n",
    "        def monitor_tool(user_rank: Union[str, int]) -> str:\n",
    "            return self.run(user_rank)\n",
    "\n",
    "        tool = CrewTool(\n",
    "            name=\"Progress Monitor Tool\",\n",
    "            description=\"Analyzes a user's Kaggle rank and gives motivational progress strategies.\",\n",
    "            func=monitor_tool\n",
    "        )\n",
    "\n",
    "        return CrewAIAgent(\n",
    "            role=\"Competition Progress Analyst\",\n",
    "            goal=\"Motivate users by analyzing their current Kaggle rank and helping them improve\",\n",
    "            backstory=\"You're a motivational and analytical AI trained to assess users’ competition rankings and give them clear steps to climb higher.\",\n",
    "            tools=[tool],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def get_autogen_agent(self, llm_config: Optional[dict] = None) -> AutoGenAssistant:\n",
    "        \"\"\"\n",
    "    Returns an AutoGen AssistantAgent capable of reasoning about user progress.\n",
    "    `llm_config` should include 'config_list', 'temperature', etc. if needed.\n",
    "    Uses a free/open model by default.\n",
    "    \"\"\"\n",
    "        if llm_config is None:\n",
    "            llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "            \"model\": \"llama3\",\n",
    "            \"api_base\": \"http://localhost:11434/v1\",\n",
    "            \"api_key\": \"ollama\", \n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "\n",
    "        return AutoGenAssistant(\n",
    "        name=self.name,\n",
    "        llm_config=llm_config,\n",
    "         memory=self.memory,\n",
    "        system_message=(\n",
    "            \"You are a competition progress analyst AI. Given a user’s rank in a Kaggle competition, \"\n",
    "            \"your job is to classify their progress bracket, offer motivational feedback, and suggest improvements.\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Dict, Any\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "\n",
    "class ErrorDiagnosisAgent:\n",
    "    def __init__(self, llm: BaseChatModel):\n",
    "        self.name = \"ErrorDiagnosisAgent\"\n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a Kaggle competition error analysis expert.\n",
    "\n",
    "The user encountered the following error or unexpected behavior:\n",
    "\"{error_description}\"\n",
    "\n",
    "Their experience level is: {user_level}\n",
    "\n",
    "Please:\n",
    "1. Explain what this error likely means in simple terms.\n",
    "2. Suggest probable root causes.\n",
    "3. Recommend debugging steps or fixes.\n",
    "4. If the issue is unclear, propose questions they can ask on Kaggle to get help.\n",
    "\n",
    "Respond in a clear, {tone} tone suitable for their experience.\n",
    "\"\"\")\n",
    "\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def run(self, structured_query: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        error_description = structured_query.get(\"query\", \"\")\n",
    "        metadata = structured_query.get(\"metadata\", {})\n",
    "        user_level = metadata.get(\"user_level\", \"intermediate\")\n",
    "        tone = metadata.get(\"tone\", \"neutral\")\n",
    "\n",
    "        try:\n",
    "            response = self.chain.run({\n",
    "                \"error_description\": error_description,\n",
    "                \"user_level\": user_level,\n",
    "                \"tone\": tone\n",
    "            })\n",
    "\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"agent_name\": self.name,\n",
    "                \"response\": f\"[Error in {self.name}] {str(e)}\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f455c",
   "metadata": {},
   "source": [
    "langgraph integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint import MemoryCheckpoint\n",
    "from typing import TypedDict, List, Optional\n",
    "#from langgraph.checkpoint import MemorySaver\n",
    "memory = DictCheckpointer()\n",
    "from graph_workflow import compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the top of your orchestration file:\n",
    "competition_summary_agent = CompetitionSummaryAgent()\n",
    "notebook_explainer_agent = NotebookExplainerAgent()\n",
    "discussion_helper_agent = DiscussionHelperAgent()\n",
    "error_diagnosis_agent = ErrorDiagnosisAgent()\n",
    "\n",
    "class OrchestratorState(TypedDict, total=False):\n",
    "    structured_query: Dict[str, Any]\n",
    "    intent: Optional[str]\n",
    "    agent_outputs: List[str]\n",
    "    final_response: Optional[str]\n",
    "    memory: Optional[Dict[str, Any]]  # Only if memory is added later\n",
    "    tone: Optional[str]\n",
    "    user_level: Optional[str]\n",
    "\n",
    "baseGraph = StateGraph(OrchestratorState)\n",
    "\n",
    "def preprocessing_node(state):\n",
    "    original_query = state.get(\"original_query\", \"\")\n",
    "    # Call your preprocessing function (returns dict with cleaned_query, tokens, etc.)\n",
    "    processed = preprocess_query(original_query)\n",
    "    # Update state with new fields\n",
    "    state[\"cleaned_query\"] = processed[\"cleaned_query\"]\n",
    "    state[\"tokens\"] = processed.get(\"tokens\", [])\n",
    "    state[\"metadata\"] = processed.get(\"metadata\", {})\n",
    "    return state\n",
    "\n",
    "orchestrator = ExpertSystemOrchestrator()\n",
    "\n",
    "def router_node(state):\n",
    "    route_result = orchestrator.route_to_agents(state[\"structured_query\"])\n",
    "    state[\"agent_outputs\"] = [route_result]\n",
    "    state[\"intent\"] = route_result.get(\"intent\")\n",
    "    return state\n",
    "\n",
    "def competition_summary_node(state):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in [\"data\", \"overview\"]:\n",
    "        response = competition_summary_agent.run(state[\"structured_query\"])\n",
    "        state[\"agent_outputs\"].append(response)\n",
    "    return state\n",
    "\n",
    "def notebook_explainer_node(state):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in [\"code\", \"model\"]:\n",
    "        response = notebook_explainer_agent.run(state[\"structured_query\"])\n",
    "        state[\"agent_outputs\"].append(response)\n",
    "    return state\n",
    "\n",
    "def discussion_helper_node(state):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in [\"discussion\"]:\n",
    "        response = discussion_helper_agent.run(state[\"structured_query\"])\n",
    "        state[\"agent_outputs\"].append(response)\n",
    "    return state\n",
    "\n",
    "def error_diagnosis_node(state):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in [\"error\", \"error_diagnosis\", \"bug\", \"troubleshooting\"]:\n",
    "        response = error_diagnosis_agent.run(state[\"structured_query\"])\n",
    "        state[\"agent_outputs\"].append(response)\n",
    "    return state\n",
    "def aggregation_node(state):\n",
    "    agent_feedback = state.get(\"agent_outputs\", [])\n",
    "\n",
    "    # Sanity check: if no agent feedback, skip aggregation\n",
    "    if not agent_feedback:\n",
    "        state[\"final_response\"] = \"No relevant agent responses to aggregate.\"\n",
    "        return state\n",
    "\n",
    "    # Run the orchestrator's aggregation logic (usually LLM-based)\n",
    "    aggregate_response = orchestrator.aggregate_response(agent_feedback)\n",
    "\n",
    "    # Store final result in the LangGraph state\n",
    "    state[\"final_response\"] = aggregate_response\n",
    "    return state\n",
    "\n",
    "\n",
    "# adding nodes\n",
    "\n",
    "baseGraph.add_node(\"competition_summary\", competition_summary_node)\n",
    "baseGraph.add_node(\"notebook_explainer\", notebook_explainer_node)\n",
    "baseGraph.add_node(\"discussion_helper\", discussion_helper_node)\n",
    "baseGraph.add_node(\"error_diagnosis\", error_diagnosis_node)\n",
    "baseGraph.add_node(\"aggregation\", aggregation_node)\n",
    "\n",
    "# conditional edges\n",
    "def route_by_intent(state) -> str:\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    router_dict = {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\"\n",
    "    }\n",
    "    return router_dict.get(intent, \"aggregation\")  # default to aggregation if not found\n",
    "baseGraph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_by_intent,\n",
    "    {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# sequential edges\n",
    "baseGraph.add_edge(START, \"preprocessing\")\n",
    "baseGraph.add_edge(\"preprocessing\", \"router\")\n",
    "baseGraph.add_edge(\"competition_summary\", \"aggregation\")\n",
    "baseGraph.add_edge(\"notebook_explainer\", \"aggregation\")\n",
    "baseGraph.add_edge(\"discussion_helper\", \"aggregation\")\n",
    "baseGraph.add_edge(\"error_diagnosis\", \"aggregation\")\n",
    "baseGraph.add_edge(\"aggregation\", END)\n",
    "\n",
    "baseGraph = baseGraph.compile(checkpointer=memory)\n",
    "compiled_graph = baseGraph\n",
    "\n",
    "# Visualize the graph (this will display in a Jupyter notebook or save to a file)\n",
    "compiled_graph.visualize(path=\"my_graph.png\")  # Saves as PNG\n",
    "# Or, in a notebook, just:\n",
    "compiled_graph.visualize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "from typing import TypedDict, Dict, Any, Optional, List\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "\n",
    "# === Agent Definitions ===\n",
    "competition_summary_agent = CompetitionSummaryAgent()\n",
    "notebook_explainer_agent = NotebookExplainerAgent()\n",
    "discussion_helper_agent = DiscussionHelperAgent()\n",
    "error_diagnosis_agent = ErrorDiagnosisAgent()  # You can define this however you want\n",
    "\n",
    "\n",
    "# === State Definition ===\n",
    "class StructuredQuery(TypedDict, total=False):\n",
    "    intent: str\n",
    "    sub_intents: List[str]\n",
    "    input_references: List[str]\n",
    "    reasoning_style: Optional[str]  # e.g., \"multi-hop\", \"timeline\", \"iterative\"\n",
    "    preferred_agents: List[str]\n",
    "    metadata_flags: Dict[str, Any]\n",
    "\n",
    "# === Orchestrator and Routing ===\n",
    "orchestrator = ExpertSystemOrchestrator()\n",
    "\n",
    "# === Node: Preprocessing ===\n",
    "def preprocessing_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    original_query = state.get(\"original_query\", \"\")\n",
    "    processed = preprocess_query(original_query)\n",
    "    state.update({\n",
    "        \"cleaned_query\": processed[\"cleaned_query\"],\n",
    "        \"tokens\": processed.get(\"tokens\", []),\n",
    "        \"metadata\": processed.get(\"metadata\", {})\n",
    "    })\n",
    "    return state\n",
    "\n",
    "# === Node: Routing ===\n",
    "def router_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    structured = state.get(\"structured_query\", {})\n",
    "    route_result = orchestrator.route_to_agents(structured)\n",
    "    intent = route_result.get(\"intent\")\n",
    "    state[\"intent\"] = intent\n",
    "    state.setdefault(\"reasoning_trace\", []).append(f\"Router identified intent: {intent}\")\n",
    "    return state\n",
    "\n",
    "# === Generic Agent Runner ===\n",
    "def run_agent_if_intent_matches(state, valid_intents, agent, agent_name):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in valid_intents:\n",
    "        response = agent.run(state[\"structured_query\"])\n",
    "        state.setdefault(\"agent_outputs\", []).append(response)\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"{agent_name} executed for intent: {intent}\")\n",
    "    return state\n",
    "\n",
    "# === Agent Nodes ===\n",
    "def competition_summary_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"data\", \"overview\"], competition_summary_agent, \"CompetitionSummaryAgent\")\n",
    "\n",
    "def notebook_explainer_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"code\", \"model\"], notebook_explainer_agent, \"NotebookExplainerAgent\")\n",
    "\n",
    "def discussion_helper_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"discussion\"], discussion_helper_agent, \"DiscussionHelperAgent\")\n",
    "\n",
    "def error_diagnosis_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"error\", \"bug\", \"troubleshooting\"], error_diagnosis_agent, \"ErrorDiagnosisAgent\")\n",
    "\n",
    "# === Reasoning Node ===\n",
    "def reasoning_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"\n",
    "    Black-box node that wraps CrewAI multi-agent reasoning.\n",
    "    Expects state to contain 'original_query' and optionally 'metadata'.\n",
    "    Returns updated state with 'crew_result'.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"original_query\", \"\")\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    # You must have a MultiAgentReasoningOrchestrator instance available as 'multiagent_orchestrator'\n",
    "    try:\n",
    "        result = MultiAgentReasoningOrchestrator.run(user_query, metadata)\n",
    "        state[\"crew_result\"] = result\n",
    "        state.setdefault(\"reasoning_trace\", []).append(\"Multi-agent reasoning node executed.\")\n",
    "        # Optionally, you can append the result to agent_outputs for aggregation\n",
    "        state.setdefault(\"agent_outputs\", []).append(result)\n",
    "    except Exception as e:\n",
    "        state[\"crew_result\"] = {\"error\": str(e)}\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"Multi-agent reasoning node failed: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "# === Aggregation Node ===\n",
    "def aggregation_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    responses = state.get(\"agent_outputs\", [])\n",
    "    if not responses:\n",
    "        state[\"final_response\"] = \"No relevant agent responses to aggregate.\"\n",
    "        return state\n",
    "    final = orchestrator.aggregate_response(responses)\n",
    "    state[\"final_response\"] = final\n",
    "    return state\n",
    "\n",
    "# === Intent Routing Function ===\n",
    "def route_by_intent(state: OrchestratorState) -> str:\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    router_dict = {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\",\n",
    "        \"progress\": \"reasoning_node\",\n",
    "        \"strategy\": \"reasoning_node\",\n",
    "        \"multi-hop\": \"reasoning_node\"\n",
    "    }\n",
    "    return router_dict.get(intent, \"reasoning_node\")  # fallback to reasoning\n",
    "\n",
    "# === Build Graph ===\n",
    "baseGraph = StateGraph(OrchestratorState)\n",
    "\n",
    "# Nodes\n",
    "baseGraph.add_node(\"preprocessing\", preprocessing_node)\n",
    "baseGraph.add_node(\"router\", router_node)\n",
    "baseGraph.add_node(\"competition_summary\", competition_summary_node)\n",
    "baseGraph.add_node(\"notebook_explainer\", notebook_explainer_node)\n",
    "baseGraph.add_node(\"discussion_helper\", discussion_helper_node)\n",
    "baseGraph.add_node(\"error_diagnosis\", error_diagnosis_node)\n",
    "baseGraph.add_node(\"reasoning_node\", reasoning_node)\n",
    "baseGraph.add_node(\"aggregation\", aggregation_node)\n",
    "\n",
    "# Edges\n",
    "baseGraph.add_edge(START, \"preprocessing\")\n",
    "baseGraph.add_edge(\"preprocessing\", \"router\")\n",
    "\n",
    "baseGraph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_by_intent,\n",
    "    {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\",\n",
    "        \"progress\": \"reasoning_node\",\n",
    "        \"strategy\": \"reasoning_node\",\n",
    "        \"multi-hop\": \"reasoning_node\"\n",
    "    }\n",
    "    # fallback handled in router_dict above\n",
    ")\n",
    "\n",
    "# All agent nodes converge to aggregation\n",
    "baseGraph.add_edge(\"competition_summary\", \"aggregation\")\n",
    "baseGraph.add_edge(\"notebook_explainer\", \"aggregation\")\n",
    "baseGraph.add_edge(\"discussion_helper\", \"aggregation\")\n",
    "baseGraph.add_edge(\"error_diagnosis\", \"aggregation\")\n",
    "baseGraph.add_edge(\"reasoning_node\", \"aggregation\")\n",
    "baseGraph.add_edge(\"aggregation\", END)\n",
    "\n",
    "# Compile\n",
    "compiled_graph = baseGraph.compile(checkpointer=memory)\n",
    "\n",
    "# Optional Visualization\n",
    "compiled_graph.visualize(path=\"my_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336872e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, Optional, List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.router import RouterChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from graph_workflow import compiled_graph\n",
    "from null import (\n",
    "    CompetitionSummaryAgent, TimelineCoachAgent, NotebookExplainerAgent,\n",
    "    CodeFeedbackAgent, MultiHopReasoningAgent, ErrorDiagnosisAgent,\n",
    "    DiscussionHelperAgent, ProgressMonitorAgent\n",
    ")\n",
    "\n",
    "# Define capability registry (unchanged, assumed already defined)\n",
    "AGENT_CAPABILITY_REGISTRY = { \n",
    "    \"NotebookExplainerAgent\": {\n",
    "        \"type\": \"LangGraph\",\n",
    "        \"capabilities\": [\"code_review\", \"model_explanation\", \"strategy_extraction\"],\n",
    "        \"reasoning_styles\": [\"default\", \"stepwise\"],\n",
    "        \"tags\": [\"code-aware\", \"code-notebook-explanation\"]\n",
    "    },\n",
    "    \"ProgressMonitorAgent\": {\n",
    "        \"type\": \"CrewAI\",\n",
    "        \"capabilities\": [\"strategy\", \"progress_tracking\", \"leaderboard_analysis\"],\n",
    "        \"reasoning_styles\": [\"multi-hop\", \"timeline\"],\n",
    "        \"tags\": [\"metadata-aware\", \"can-coordinate\"]\n",
    "    },\n",
    "    \"CompetitionSummaryAgent\": {\n",
    "        \"type\": \"LangGraph\",\n",
    "        \"capabilities\": [\"competition_overview\", \"rules_explanation\", \"leaderboard_summary\"],\n",
    "        \"reasoning_styles\": [\"summary\", \"default\"],\n",
    "        \"tags\": [\"overview\", \"competition\"]\n",
    "    },\n",
    "    \"TimelineCoachAgent\": {\n",
    "        \"type\": \"CrewAI\",\n",
    "        \"capabilities\": [\"timeline_planning\", \"milestone_advice\", \"progress_acceleration\"],\n",
    "        \"reasoning_styles\": [\"timeline\", \"stepwise\"],\n",
    "        \"tags\": [\"planning\", \"time-management\"]\n",
    "    },\n",
    "    \"CodeFeedbackAgent\": {\n",
    "        \"type\": \"CrewAI\",\n",
    "        \"capabilities\": [\"code_review\", \"bug_detection\", \"performance_suggestion\"],\n",
    "        \"reasoning_styles\": [\"critique\", \"stepwise\"],\n",
    "        \"tags\": [\"code-aware\", \"ml-best-practices\"]\n",
    "    },\n",
    "    \"MultiHopReasoningAgent\": {\n",
    "        \"type\": \"CrewAI\",\n",
    "        \"capabilities\": [\"multi_hop_reasoning\", \"complex_query\", \"cross_agent_coordination\"],\n",
    "        \"reasoning_styles\": [\"multi-hop\", \"stepwise\"],\n",
    "        \"tags\": [\"reasoning\", \"coordination\"]\n",
    "    },\n",
    "    \"ErrorDiagnosisAgent\": {\n",
    "        \"type\": \"LangGraph\",\n",
    "        \"capabilities\": [\"error_diagnosis\", \"bug_detection\", \"troubleshooting\"],\n",
    "        \"reasoning_styles\": [\"stepwise\", \"default\"],\n",
    "        \"tags\": [\"debugging\", \"code-aware\"]\n",
    "    },\n",
    "    \"DiscussionHelperAgent\": {\n",
    "        \"type\": \"LangGraph\",\n",
    "        \"capabilities\": [\"discussion_clarification\", \"question_generation\", \"forum_navigation\"],\n",
    "        \"reasoning_styles\": [\"default\", \"summary\"],\n",
    "        \"tags\": [\"communication\", \"community\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# For AutoGen, use the same agent definitions as CrewAI\n",
    "AUTOGEN_AGENT_CAPABILITY_REGISTRY = {\n",
    "    agent: details for agent, details in AGENT_CAPABILITY_REGISTRY.items() if details[\"type\"] == \"CrewAI\"\n",
    "}\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\n",
    "Given a user query, route it to the correct agent using this structure:\n",
    "\n",
    "- intent: main goal\n",
    "- sub_intents: list of secondary goals\n",
    "- input_references: e.g., code, data, notebook\n",
    "- reasoning_style: e.g., step_by_step, critique\n",
    "- preferred_agents: list of agents user prefers\n",
    "- metadata_flags: e.g., beginner, urgent\n",
    "- confidence_score: 0.0 to 1.0\n",
    "- uncertain_parse: true/false\n",
    "- query_mode: debugging, exploration, planning, etc.\n",
    "\n",
    "Respond only in JSON.\n",
    "\"\"\"\n",
    "\n",
    "class ExpertSystemOrchestratorLangGraph:\n",
    "    def __init__(self):\n",
    "        self.routing_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "\n",
    "        self.destinations = {\n",
    "            \"Competition Summary\": {\n",
    "                \"name\": \"competition_summary\",\n",
    "                \"description\": \"Handles general questions about the competition goal, rules, or overview.\",\n",
    "                \"chain\": CompetitionSummaryAgent()\n",
    "            },\n",
    "            \"Timeline Coach\": {\n",
    "                \"name\": \"timeline_coach\",\n",
    "                \"description\": \"Helps the user plan realistic timelines and project checkpoints.\",\n",
    "                \"chain\": TimelineCoachAgent()\n",
    "            },\n",
    "            \"Notebook Explainer\": {\n",
    "                \"name\": \"notebook_explainer\",\n",
    "                \"description\": \"Explains code snippets and pinned notebook logic to beginners.\",\n",
    "                \"chain\": NotebookExplainerAgent()\n",
    "            },\n",
    "            \"Code Feedback\": {\n",
    "                \"name\": \"code_feedback\",\n",
    "                \"description\": \"Reviews user code and offers suggestions for improvement.\",\n",
    "                \"chain\": CodeFeedbackAgent()\n",
    "            },\n",
    "            \"Multi-hop Reasoning\": {\n",
    "                \"name\": \"multi_hop_reasoning\",\n",
    "                \"description\": \"Answers questions requiring multiple sources or steps to resolve.\",\n",
    "                \"chain\": MultiHopReasoningAgent()\n",
    "            },\n",
    "            \"Error Diagnosis\": {\n",
    "                \"name\": \"error_diagnosis\",\n",
    "                \"description\": \"Helps identify and fix errors or unexpected behaviors in code.\",\n",
    "                \"chain\": ErrorDiagnosisAgent()\n",
    "            },\n",
    "            \"Discussion Helper\": {\n",
    "                \"name\": \"discussion_helper\",\n",
    "                \"description\": \"Formulates meaningful questions or clarifies confusing forum replies.\",\n",
    "                \"chain\": DiscussionHelperAgent()\n",
    "            },\n",
    "            \"Progress Monitor\": {\n",
    "                \"name\": \"progress_monitor\",\n",
    "                \"description\": \"Analyzes the user’s past progress and advises next steps.\",\n",
    "                \"chain\": ProgressMonitorAgent()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.destination_chains = {\n",
    "            key: value[\"chain\"] for key, value in self.destinations.items()\n",
    "        }\n",
    "\n",
    "        router_prompt = PromptTemplate.from_template(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "\n",
    "        self.router_chain = RouterChain.from_names_and_descriptions(\n",
    "            names_and_descriptions=[\n",
    "                (value[\"name\"], value[\"description\"]) for value in self.destinations.values()\n",
    "            ],\n",
    "            llm=self.routing_llm,\n",
    "            prompt=router_prompt\n",
    "        )\n",
    "\n",
    "        self.aggregation_chain = LLMChain(\n",
    "            llm=ChatOpenAI(\n",
    "                model_name=\"mistralai/Mixtral-8x7b-Instruct-v0.1\",\n",
    "                openai_api_base=\"https://api.together.xyz/v1\",\n",
    "                openai_api_key=\"...\"\n",
    "            ),\n",
    "            prompt=PromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                You are a smart AI orchestrator. Several expert agents have responded to a user's question.\n",
    "\n",
    "                Here are their responses:\n",
    "                {agent_responses}\n",
    "\n",
    "                Based on these, synthesize a coherent and helpful answer. Make sure to:\n",
    "                - Combine overlapping insights\n",
    "                - Prioritize concrete suggestions\n",
    "                - Omit redundant explanations\n",
    "                - Be concise, clear, and actionable\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.graph = compiled_graph\n",
    "\n",
    "    def route_to_agents(self, original_query: str) -> Dict[str, Any]:\n",
    "        few_shot_prompt = f\"\"\"\n",
    "You are an expert query router. Given a user query, extract:\n",
    "\n",
    "- intent\n",
    "- sub_intents\n",
    "- input_references\n",
    "- reasoning_style\n",
    "- preferred_agents\n",
    "- metadata_flags\n",
    "- confidence_score\n",
    "- uncertain_parse\n",
    "- query_mode\n",
    "\n",
    "Q: \"{original_query}\"\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "        router_output = self.routing_llm(few_shot_prompt)\n",
    "\n",
    "        try:\n",
    "            structured_query = json.loads(router_output)\n",
    "        except Exception as e:\n",
    "            structured_query = {\n",
    "                \"intent\": \"reasoning\",\n",
    "                \"sub_intents\": [],\n",
    "                \"input_references\": [],\n",
    "                \"reasoning_style\": \"default\",\n",
    "                \"preferred_agents\": [],\n",
    "                \"metadata_flags\": {},\n",
    "                \"confidence_score\": 0.0,\n",
    "                \"uncertain_parse\": True,\n",
    "                \"query_mode\": \"exploration\",\n",
    "                \"llm_parse_error\": str(e),\n",
    "                \"raw_output\": router_output\n",
    "            }\n",
    "\n",
    "        intent = structured_query.get(\"intent\", \"reasoning\")\n",
    "\n",
    "        if structured_query.get(\"uncertain_parse\", False):\n",
    "            structured_query[\"destination_chain_name\"] = \"multi_hop_reasoning\"\n",
    "        else:\n",
    "            structured_query[\"destination_chain_name\"] = (\n",
    "                intent if intent in self.destination_chains else None\n",
    "            )\n",
    "\n",
    "        return structured_query\n",
    "\n",
    "    def find_agents_by_subintent(\n",
    "        self,\n",
    "        subintent: str,\n",
    "        reasoning_style: Optional[str] = None,\n",
    "        include_scores: bool = True,\n",
    "        min_score_threshold: float = 0.3\n",
    "    ) -> List[Dict[str, float]]:\n",
    "        matches = []\n",
    "\n",
    "        for agent_name, details in AGENT_CAPABILITY_REGISTRY.items():\n",
    "            score = 0.0\n",
    "            debug_reason = []\n",
    "\n",
    "            if subintent in details.get(\"capabilities\", []):\n",
    "                score += 0.6\n",
    "                debug_reason.append(\"capability match (+0.6)\")\n",
    "\n",
    "            if reasoning_style and reasoning_style in details.get(\"reasoning_styles\", []):\n",
    "                score += 0.3\n",
    "                debug_reason.append(\"reasoning style match (+0.3)\")\n",
    "\n",
    "            if subintent in details.get(\"tags\", []):\n",
    "                score += 0.2\n",
    "                debug_reason.append(\"tag match (+0.2)\")\n",
    "\n",
    "            if score >= min_score_threshold:\n",
    "                matches.append({\n",
    "                    \"agent\": agent_name,\n",
    "                    \"score\": round(score, 2),\n",
    "                    \"explanation\": \", \".join(debug_reason)\n",
    "                })\n",
    "\n",
    "        matches.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return matches\n",
    "\n",
    "    def explain_agent_routing(self, structured_query: Dict[str, Any]) -> List[str]:\n",
    "        subintents = structured_query.get(\"sub_intents\", [])\n",
    "        style = structured_query.get(\"reasoning_style\")\n",
    "        explanations = []\n",
    "\n",
    "        for subintent in subintents:\n",
    "            matches = self.find_agents_by_subintent(subintent, style)\n",
    "            for match in matches:\n",
    "                explanations.append(f\"{match['agent']} matched for {subintent} ({match['explanation']})\")\n",
    "\n",
    "        return explanations\n",
    "\n",
    "    def aggregate_response(self, agent_responses: List[Dict[str, str]]) -> str:\n",
    "        responses_text = \"\\n\\n\".join(\n",
    "            [f\"{i+1}. {resp['agent_name']}: {resp['response']}\" for i, resp in enumerate(agent_responses)]\n",
    "        )\n",
    "        return self.aggregation_chain.run(agent_responses=responses_text)\n",
    "\n",
    "    def handle_query(self, user_query: str, debug: bool = False) -> str:\n",
    "        initial_state = {\"original_query\": user_query}\n",
    "        final_state = self.graph.invoke(initial_state, return_intermediate_steps=debug)\n",
    "        return final_state if debug else final_state.get(\"final_response\", \"[No response generated]\")\n",
    "\n",
    "    def run_graph_debug(self, user_query: str):\n",
    "        return self.handle_query(user_query, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f948ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import metadata\n",
    "from graph_workflow import compiled_graph\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ExpertSystemOrchestratorLangGraph:\n",
    "    def __init__(self):\n",
    "        self.routing_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "        # === Step 1: Define agent descriptions ===\n",
    "        self.destinations = {\n",
    "            \"Competition Summary\": {\n",
    "                \"name\": \"competition_summary\",\n",
    "                \"description\": \"Handles general questions about the competition goal, rules, or overview.\",\n",
    "                \"chain\": CompetitionSummaryAgent()\n",
    "            },\n",
    "            \"Timeline Coach\": {\n",
    "                \"name\": \"timeline_coach\",\n",
    "                \"description\": \"Helps the user plan realistic timelines and project checkpoints.\",\n",
    "                \"chain\": TimelineCoachAgent()\n",
    "            },\n",
    "            \"Notebook Explainer\": {\n",
    "                \"name\": \"notebook_explainer\",\n",
    "                \"description\": \"Explains code snippets and pinned notebook logic to beginners.\",\n",
    "                \"chain\": NotebookExplainerAgent()\n",
    "            },\n",
    "            \"Code Feedback\": {\n",
    "                \"name\": \"code_feedback\",\n",
    "                \"description\": \"Reviews user code and offers suggestions for improvement.\",\n",
    "                \"chain\": CodeFeedbackAgent()\n",
    "            },\n",
    "            \"Multi-hop Reasoning\": {\n",
    "                \"name\": \"multi_hop_reasoning\",\n",
    "                \"description\": \"Answers questions requiring multiple sources or steps to resolve.\",\n",
    "                \"chain\": MultiHopReasoningAgent()\n",
    "            },\n",
    "            \"Error Diagnosis\": {\n",
    "                \"name\": \"error_diagnosis\",\n",
    "                \"description\": \"Helps identify and fix errors or unexpected behaviors in code.\",\n",
    "                \"chain\": ErrorDiagnosisAgent()\n",
    "            },\n",
    "            \"Discussion Helper\": {\n",
    "                \"name\": \"discussion_helper\",\n",
    "                \"description\": \"Formulates meaningful questions or clarifies confusing forum replies.\",\n",
    "                \"chain\": DiscussionHelperAgent()\n",
    "            },\n",
    "            \"Progress Monitor\": {\n",
    "                \"name\": \"progress_monitor\",\n",
    "                \"description\": \"Analyzes the user’s past progress and advises next steps.\",\n",
    "                \"chain\": ProgressMonitorAgent()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # === Step 2: Build destination chains and router ===\n",
    "        self.destination_chains = {\n",
    "            key: value[\"chain\"] for key, value in self.destinations.items()\n",
    "        }\n",
    "\n",
    "        router_prompt = PromptTemplate.from_template(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "        self.router_chain = RouterChain.from_names_and_descriptions(\n",
    "            names_and_descriptions=[\n",
    "                (value[\"name\"], value[\"description\"]) for value in self.destinations.values()\n",
    "            ],\n",
    "            llm=self.routing_llm,\n",
    "            prompt=router_prompt\n",
    "        )\n",
    "\n",
    "        self.aggregation_chain = LLMChain(\n",
    "            prompt=PromptTemplate(\n",
    "                \"\"\"\n",
    "                You are a smart AI orchestrator. Several expert agents have responded to a user's question.\n",
    "\n",
    "                Here are their responses:\n",
    "                {agent_responses}\n",
    "\n",
    "                Based on these, synthesize a coherent and helpful answer. Make sure to:\n",
    "                - Combine overlapping insights\n",
    "                - Prioritize concrete suggestions\n",
    "                - Omit redundant explanations\n",
    "                - Be concise, clear, and actionable\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self.aggregation_llm = ChatOpenAI(\n",
    "            model_name=\"mistralai/Mixtral-8x7b-Instruct-v0.1\",\n",
    "            openai_api_base=\"https://api.together.xyz/v1\",\n",
    "            openai_api_key=\"...\"\n",
    "        )\n",
    "\n",
    "        self.graph = compiled_graph\n",
    "\n",
    "    \n",
    "    def aggregate_response(self, agent_responses: list[dict]) -> str:\n",
    "        \"\"\"\n",
    "        Combine multiple agent responses into a coherent reply for the user using an LLM.\n",
    "        Each response dict is expected to have a 'response' key and 'agent_name'.\n",
    "        \"\"\"\n",
    "        responses_text = \"\\n\\n\".join(\n",
    "            [f\"{i+1}. {resp.get('agent_name', 'Agent')}: {resp.get('response', '')}\" for i, resp in enumerate(agent_responses)]\n",
    "        )\n",
    "        return self.aggregation_chain.run(agent_responses=responses_text)\n",
    "\n",
    "    def handle_query(self, user_query: str) -> str:\n",
    "        initial_state = {\n",
    "            \"original_query\": user_query,\n",
    "        }\n",
    "        final_state = self.graph.invoke(initial_state)\n",
    "        return final_state.get(\"final_response\", \"[No response generated]\")\n",
    "\n",
    "    def trace_query(self, user_query: str):\n",
    "        \"\"\"\n",
    "        Returns a full LangGraph step trace along with the final output.\n",
    "        Useful for debugging and transparency.\n",
    "        \"\"\"\n",
    "        initial_state = {\"original_query\": user_query}\n",
    "        result = self.graph.invoke(initial_state, return_intermediate_steps=True)\n",
    "        trace = result.get(\"intermediate_steps\", [])\n",
    "        final_output = result.get(\"final_response\", \"[No response generated]\")\n",
    "        return {\n",
    "            \"trace\": trace,\n",
    "            \"final_output\": final_output\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from typing import Optional\n",
    "\n",
    "class MultiHopReasoningAgent:\n",
    "    def __init__(self, retriever: Optional[object] = None):\n",
    "        self.retriever = retriever\n",
    "\n",
    "        self.reasoning_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are a multi-hop reasoning expert for data science competitions.\n",
    "\n",
    "            The user's question may require pulling together multiple pieces of information or logic steps.\n",
    "\n",
    "            Carefully think step-by-step and explain your reasoning process before giving a final answer.\n",
    "\n",
    "            USER QUESTION:\n",
    "            {question}\n",
    "\n",
    "            Start your reasoning below:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o\", temperature=0.3\n",
    "        )\n",
    "\n",
    "        self.reasoning_chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.reasoning_prompt\n",
    "        )\n",
    "\n",
    "def run(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Handles multi-hop reasoning by combining optional retrieval and chain-of-thought reasoning.\n",
    "        \"\"\"\n",
    "        retrieved_docs = \"\"\n",
    "        if self.retriever:\n",
    "            try:\n",
    "                # Pull top 3 relevant documents\n",
    "                docs = self.retriever.get_relevant_documents(question)\n",
    "                retrieved_docs = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "                question = f\"{question}\\n\\n[Retrieved Context]\\n{retrieved_docs}\"\n",
    "            except Exception:\n",
    "                pass  # If retriever fails, continue with original question\n",
    "\n",
    "        return self.reasoning_chain.run({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b955773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class ProgressMonitorAgent:\n",
    "    def __init__(self):\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are a competition progress analyst AI.\n",
    "\n",
    "            A user is participating in a Kaggle-like data science competition and has told you their current rank: {user_rank}.\n",
    "\n",
    "            Your job is to:\n",
    "            1. Classify the user’s rank into a bracket (e.g., Top 10, Top 100, Top 500, etc.).\n",
    "            2. Provide insights on what users in higher ranks (above them) tend to do.\n",
    "            3. Suggest general strategies to help them move closer to the next tier.\n",
    "            4. Avoid giving specific answers from other notebooks.\n",
    "            5. If possible, suggest collaboration with a Notebook Explainer AI for deeper code-level comparisons (optional for now).\n",
    "\n",
    "            Respond concisely but informatively, in a motivational and analytical tone.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = LLMChain(\n",
    "            llm=ChatOpenAI(\n",
    "                model_name=\"gpt-4o\",\n",
    "                temperature=0.3\n",
    "            ),\n",
    "            prompt=self.prompt\n",
    "        )\n",
    "\n",
    "    def run(self, user_rank: str) -> str:\n",
    "        \"\"\"\n",
    "        user_rank: str or int provided by the user indicating their current rank (e.g., \"128\" or \"Top 500\")\n",
    "        \"\"\"\n",
    "        return self.chain.run({\"user_rank\": user_rank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921dcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from typing import Optional, Dict, Any\n",
    "import json\n",
    "\n",
    "import autogen\n",
    "from autogen import GroupChat, GroupChatManager, UserProxyAgent, AssistantAgent\n",
    "\n",
    "from null.multi_hop_agent import MultiHopReasoningAgent\n",
    "from null.progress_monitor_agent import ProgressMonitorAgent\n",
    "from null.code_feedback_agent import CodeFeedbackAgent\n",
    "from null.timeline_coach_agent import TimelineCoachAgent\n",
    "\n",
    "\n",
    "class AutoGenReasoningOrchestrator:\n",
    "    def __init__(self):\n",
    "        # Step 1: LLM config (can be extended to use environment config)\n",
    "        self.llm_config = {\n",
    "            \"timeout\": 600,\n",
    "            \"seed\": 42,\n",
    "            \"config_list\": autogen.config_list_from_json(\n",
    "                \"OAI_CONFIG_LIST\",\n",
    "                filter_dict={\"model\": [\"gpt-4\", \"gpt-3.5-turbo\"]},\n",
    "            ),\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "        self.memory_trace = {\n",
    "            \"code_review\": [],\n",
    "            \"progress_eval\": [],\n",
    "            \"reasoning\": [],\n",
    "        }\n",
    "\n",
    "\n",
    "        # Step 2: Load and wrap all agents using .get_autogen_agent()\n",
    "        self.user_proxy = UserProxyAgent(\n",
    "            name=\"Admin\",\n",
    "            system_message=\"You are the human coordinator. Say 'TERMINATE' to end.\",\n",
    "            code_execution_config={\"use_docker\": False, \"work_dir\": \"coding\"},\n",
    "            human_input_mode=\"ALWAYS\",\n",
    "            llm_config=self.llm_config,\n",
    "        )\n",
    "\n",
    "        self.multi_hop_agent = MultiHopReasoningAgent().get_autogen_agent(self.llm_config)\n",
    "        self.progress_monitor_agent = ProgressMonitorAgent().get_autogen_agent(self.llm_config)\n",
    "        self.code_feedback_agent = CodeFeedbackAgent().get_autogen_agent(self.llm_config)\n",
    "        self.timeline_coach_agent = TimelineCoachAgent().get_autogen_agent(self.llm_config)\n",
    "        self.round_counter = 0\n",
    "        self.agent_add_history = []\n",
    "        self.recent_discussion = []\n",
    "    def build_groupchats(self) -> Dict[str, GroupChatManager]:\n",
    "        def build_with_memory(agents, memory_key):\n",
    "            # Create a memory preamble message\n",
    "            memory_messages = self.memory_trace[memory_key]\n",
    "            memory_summary = \"\\n\".join(memory_messages[-5:])  # only last 5 turns\n",
    "            if memory_summary:\n",
    "                system_msg = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"[MEMORY CONTEXT]\\n{memory_summary}\"\n",
    "                }\n",
    "            else:\n",
    "                system_msg = {}\n",
    "\n",
    "            chat = GroupChat(\n",
    "                agents=agents,\n",
    "                messages=[autogen.Message(**system_msg)] if system_msg else [],\n",
    "                max_round=10,\n",
    "            )\n",
    "            return GroupChatManager(groupchat=chat, llm_config=self.llm_config)\n",
    "\n",
    "        return {\n",
    "            \"code_review\": build_with_memory([self.user_proxy, self.code_feedback_agent], \"code_review\"),\n",
    "            \"progress_eval\": build_with_memory(\n",
    "                [self.user_proxy, self.progress_monitor_agent, self.timeline_coach_agent], \"progress_eval\"\n",
    "            ),\n",
    "            \"reasoning\": build_with_memory([self.user_proxy, self.multi_hop_agent], \"reasoning\"),\n",
    "        }\n",
    "\n",
    "    def custom_speaker_selection(self, last_speaker, last_message, groupchat, selector):\n",
    "        self.round_counter += 1\n",
    "        cooldown_window = 3  # how many rounds to avoid re-adding an agent\n",
    "\n",
    "        last_message_content = last_message.get(\"content\", \"\")\n",
    "        last_speaker_name = last_message.get(\"name\", \"\")\n",
    "        print(f\"\\n[DEBUG] ⏱ Round {self.round_counter}\")\n",
    "        print(f\"[DEBUG] Last message from '{last_speaker_name}': {last_message_content}\")\n",
    "\n",
    "        current_agents = [agent.name for agent in groupchat.agents]\n",
    "        print(f\"[DEBUG] Current agents: {current_agents}\")\n",
    "\n",
    "        agent_pool = {\n",
    "            \"MultiHopReasoningAgent\": self.multi_hop_agent,\n",
    "            \"ProgressMonitorAgent\": self.progress_monitor_agent,\n",
    "            \"CodeFeedbackAgent\": self.code_feedback_agent,\n",
    "            \"TimelineCoachAgent\": self.timeline_coach_agent,\n",
    "        }\n",
    "\n",
    "        # Step 1: Classify message intent\n",
    "        # Use self.llm_config instead of undefined llm_config\n",
    "        llm_prompt = (\n",
    "            \"Classify the intent of the following message and extract any relevant entities.\\n\"\n",
    "            f\"Message: {last_message_content}\\n\"\n",
    "            \"Respond as JSON: {\\\"intent\\\": ..., \\\"entities\\\": [...]}.\"\n",
    "        )\n",
    "        try:\n",
    "            llm_response = autogen.oai.ChatCompletion.create(\n",
    "                model=self.llm_config[\"config_list\"][0][\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": llm_prompt}],\n",
    "                api_key=self.llm_config[\"config_list\"][0].get(\"api_key\"),\n",
    "                api_base=self.llm_config[\"config_list\"][0].get(\"api_base\"),\n",
    "                temperature=self.llm_config.get(\"temperature\", 0.7),\n",
    "                timeout=self.llm_config.get(\"timeout\", 600),\n",
    "            )\n",
    "            llm_content = llm_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            parsed = json.loads(llm_content)\n",
    "            intent = parsed.get(\"intent\", \"\").strip().lower()\n",
    "            entities = [e.lower() for e in parsed.get(\"entities\", [])]\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] LLM parsing failed: {e}\")\n",
    "            intent = \"\"\n",
    "            entities = []\n",
    "\n",
    "        print(f\"[DEBUG] LLM intent: {intent}, entities: {entities}\")\n",
    "\n",
    "        # Step 2: Guard against missing intent\n",
    "        if not intent:\n",
    "            print(\"[INFO] No clear intent. Skipping agent update.\")\n",
    "            return autogen.agentchat.groupchat.GroupChat.auto_select_speaker(\n",
    "                last_speaker, selector, selector.groupchat.messages, selector.groupchat.agents\n",
    "            )\n",
    "\n",
    "        # Step 3: Get required agents\n",
    "        intent_agent_map = {\n",
    "            \"code_review\": [\"CodeFeedbackAgent\"],\n",
    "            \"progress_tracking\": [\"ProgressMonitorAgent\", \"TimelineCoachAgent\"],\n",
    "            \"multi_hop_reasoning\": [\"MultiHopReasoningAgent\"],\n",
    "            \"timeline_coaching\": [\"TimelineCoachAgent\"],\n",
    "        }\n",
    "        required_agents = intent_agent_map.get(intent, [])\n",
    "\n",
    "        # Step 4: Remove agents not needed\n",
    "        for agent in groupchat.agents[:]:  # Copy to avoid mutation during loop\n",
    "            if agent.name != \"Admin\" and agent.name not in required_agents:\n",
    "                print(f\"--- Removing {agent.name} from the group chat (task complete) ---\")\n",
    "                groupchat.agents.remove(agent)\n",
    "\n",
    "        # Step 5: Add needed agents if not throttled\n",
    "        for agent_name in required_agents:\n",
    "            recently_added = any(\n",
    "                (record[0] == agent_name and record[1] == intent and self.round_counter - record[2] <= cooldown_window)\n",
    "                for record in self.agent_add_history\n",
    "            )\n",
    "\n",
    "            if agent_name not in current_agents:\n",
    "                if recently_added:\n",
    "                    print(f\"[INFO] ⛔ Skipping {agent_name} (recently added for intent '{intent}')\")\n",
    "                elif agent_name in agent_pool:\n",
    "                    agent_obj = agent_pool[agent_name]\n",
    "                    print(f\"--- ✅ Adding {agent_name} to the group chat for intent '{intent}' ---\")\n",
    "                    groupchat.agents.append(agent_obj)\n",
    "                    self.agent_add_history.append((agent_name, intent, self.round_counter))\n",
    "\n",
    "        # Step 6: Fallback if Admin is alone\n",
    "        if len(groupchat.agents) == 1:\n",
    "            print(\"[WARN] Only Admin left. Re-adding MultiHopReasoningAgent.\")\n",
    "            groupchat.agents.append(self.multi_hop_agent)\n",
    "\n",
    "        # Step 7: Print agents considered for speaker selection\n",
    "        final_candidates = [agent.name for agent in groupchat.agents]\n",
    "        print(f\"[DEBUG] 🗣 Speaker candidates for auto_select: {final_candidates}\")\n",
    "# Track recent discussion\n",
    "        if last_speaker_name and intent:\n",
    "            self.recent_discussion.append((self.round_counter, last_speaker_name, intent))\n",
    "            self.recent_discussion = self.recent_discussion[-10:]  # Limit memory\n",
    "\n",
    "# Reorder agents to prioritize recent intent responders\n",
    "        prioritized_agents = sorted(\n",
    "        groupchat.agents,\n",
    "    key=lambda agent: next(\n",
    "        (10 - (self.round_counter - rd[0])  # Higher score for recent\n",
    "         for rd in reversed(self.recent_discussion)\n",
    "         if rd[1] == agent.name and rd[2] == intent),\n",
    "        0\n",
    "    ),\n",
    "    reverse=True,\n",
    ")\n",
    "        print(f\"[DEBUG] Prioritized agent order: {[agent.name for agent in prioritized_agents]}\")\n",
    "        selector.groupchat.agents = prioritized_agents\n",
    "\n",
    "        return autogen.agentchat.groupchat.GroupChat.auto_select_speaker(\n",
    "    last_speaker, selector, selector.groupchat.messages, selector.groupchat.agents\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, user_query: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        group_managers = self.build_groupchats()\n",
    "\n",
    "        thread_tasks = [\n",
    "            (\"code_review\", group_managers[\"code_review\"], \"Please review the user's model code.\"),\n",
    "            (\"progress_eval\", group_managers[\"progress_eval\"], \"Assess the user's progress and suggest improvements.\"),\n",
    "            (\"reasoning\", group_managers[\"reasoning\"], f\"Answer the user's query: {user_query}\"),\n",
    "        ]\n",
    "\n",
    "        def run_thread(memory_key, manager, message):\n",
    "            result = self.user_proxy.initiate_chat(manager, message)\n",
    "            self.memory_trace[memory_key].append(f\"[Round {self.round_counter}] User said: {message}\")\n",
    "            self.memory_trace[memory_key].append(f\"[Round {self.round_counter}] Agent response: {result}\")\n",
    "            self.memory_trace[memory_key] = self.memory_trace[memory_key][-10:]\n",
    "\n",
    "        threads = []\n",
    "        for key, manager, message in thread_tasks:\n",
    "            t = threading.Thread(target=run_thread, args=(key, manager, message))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        return self.memory_trace\n",
    "    print(\"\\n✅ All group chats completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a428422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def find_agents_for_task(subintents: List[str], reasoning_style: str) -> List[str]:\n",
    "    matched_agents = []\n",
    "    for agent_name, props in AGENT_CAPABILITY_REGISTRY.items():\n",
    "        if any(s in props[\"capabilities\"] for s in subintents):\n",
    "            if reasoning_style in props[\"reasoning_styles\"]:\n",
    "                matched_agents.append(agent_name)\n",
    "    return matched_agents\n",
    "\n",
    "def select_backend(candidates: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Decide which execution backend to use.\n",
    "    Prioritize:\n",
    "    1. LangGraph if only LangGraph agents are matched\n",
    "    2. CrewAI if team collaboration is needed\n",
    "    3. AutoGen if highly conversational\n",
    "    \"\"\"\n",
    "    types = set(AGENT_CAPABILITY_REGISTRY[a][\"type\"] for a in candidates)\n",
    "    if types == {\"LangGraph\"}:\n",
    "        return \"LangGraph\"\n",
    "    elif \"AutoGen\" in types:\n",
    "        return \"AutoGen\"\n",
    "    else:\n",
    "        return \"CrewAI\"\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def dispatch_query(structured_query: Dict[str, Any], memory: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "    subintents = structured_query.get(\"sub_intents\", [])\n",
    "    reasoning_style = structured_query.get(\"reasoning_style\", \"default\")\n",
    "    intent = structured_query.get(\"intent\", \"unknown\")\n",
    "\n",
    "    # Step 1: Core agent selection\n",
    "    matched_agents = find_agents_for_task(subintents, reasoning_style)\n",
    "    selected_backend = select_backend(matched_agents)\n",
    "\n",
    "    # Step 2: Memory-aware adjustments\n",
    "    memory = memory or {}\n",
    "    intent_history: List[str] = memory.get(\"intent_history\", [])\n",
    "    past_queries: List[str] = memory.get(\"past_queries\", [])\n",
    "    agent_outputs_log: List[List[str]] = memory.get(\"agent_outputs_log\", [])\n",
    "\n",
    "    # Heuristic 1: Repeated intent → escalate\n",
    "    intent_repeat_count = intent_history[-5:].count(intent)\n",
    "    if intent_repeat_count >= 3 and \"MentorAgent\" not in matched_agents:\n",
    "        matched_agents.append(\"MentorAgent\")\n",
    "\n",
    "    # Heuristic 2: Too many empty agent outputs recently → fallback agent\n",
    "    recent_empty_responses = [o for o in agent_outputs_log[-3:] if not o]\n",
    "    if len(recent_empty_responses) >= 2:\n",
    "        matched_agents.append(\"MetaReasonerAgent\")\n",
    "\n",
    "    # Optional: Deduplicate\n",
    "    matched_agents = list(set(matched_agents))\n",
    "    selected_backend = select_backend(matched_agents)\n",
    "\n",
    "    # Step 3: Reason summary\n",
    "    reasoning_parts = [\n",
    "        f\"Subintents: {subintents}\",\n",
    "        f\"Reasoning style: '{reasoning_style}'\",\n",
    "        f\"Intent: '{intent}' repeated {intent_repeat_count}x\",\n",
    "        f\"Recent agent failures: {len(recent_empty_responses)}\",\n",
    "        f\"Final agents: {matched_agents}\",\n",
    "        f\"Backend: {selected_backend}\"\n",
    "    ]\n",
    "    reasoning = \" | \".join(reasoning_parts)\n",
    "\n",
    "    return {\n",
    "        \"selected_agents\": matched_agents,\n",
    "        \"selected_backend\": selected_backend,\n",
    "        \"dispatch_reason\": reasoning\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b548735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this\n",
    "# === Imports ===\n",
    "from typing import TypedDict, Dict, Any, Optional, List\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint import MemoryCheckpointer\n",
    "\n",
    "from crew_team_planner import CrewTeamPlanner\n",
    "from autogen_bridge import run_autogen_specialists\n",
    "from langgraph_execution import run_langgraph_agents  # optional for subflow\n",
    "from crewai import Crew, Task  # <-- Add Task import\n",
    "import asyncio\n",
    "from langchain.schema import BaseOutputParser\n",
    "from typing import Dict, Any\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "memory = MemoryCheckpointer()\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# === Agent Definitions ===\n",
    "competition_summary_agent = CompetitionSummaryAgent()\n",
    "notebook_explainer_agent = NotebookExplainerAgent()\n",
    "discussion_helper_agent = DiscussionHelperAgent()\n",
    "error_diagnosis_agent = ErrorDiagnosisAgent()\n",
    "\n",
    "# === State Definition ===\n",
    "class StructuredQuery(TypedDict, total=False): \n",
    "    intent: str\n",
    "    sub_intents: List[str]\n",
    "    input_references: List[str]\n",
    "    reasoning_style: Optional[str]\n",
    "    preferred_agents: List[str]\n",
    "    metadata_flags: Dict[str, Any]\n",
    "    confidence_score: Optional[float]                # NEW\n",
    "    uncertain_parse: Optional[bool]                  # NEW\n",
    "    query_mode: Optional[str]\n",
    "class OrchestratorState(TypedDict, total=False):\n",
    "    original_query: str\n",
    "    structured_query: StructuredQuery\n",
    "    cleaned_query: str\n",
    "    tokens: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    intent: Optional[str]\n",
    "    agent_outputs: List[str]\n",
    "    reasoning_trace: List[str]\n",
    "    conversation_trace: List[str]\n",
    "    final_response: Optional[str]\n",
    "    memory: Optional[Dict[str, Any]]\n",
    "    tone: Optional[str]\n",
    "    user_level: Optional[str]\n",
    "    crew_result: Optional[Any]\n",
    "\n",
    "# === Orchestrator Instances ===\n",
    "orchestrator = ExpertSystemOrchestrator()\n",
    "crewAI_orchestrator = MultiAgentReasoningOrchestrator()\n",
    "autogen_conversational_orchestrator = AutoGenReasoningOrchestrator()\n",
    "\n",
    "# === Node: Preprocessing ===\n",
    "def preprocessing_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    original_query = state.get(\"original_query\", \"\")\n",
    "    processed = preprocess_query(original_query)\n",
    "\n",
    "    # Step 1: Core preprocessing\n",
    "    state.update({\n",
    "        \"cleaned_query\": processed[\"cleaned_query\"],\n",
    "        \"tokens\": processed.get(\"tokens\", []),\n",
    "        \"metadata\": processed.get(\"metadata\", {})\n",
    "    })\n",
    "\n",
    "    # Step 2: Memory update for loop detection\n",
    "    memory = state.get(\"memory\", {})\n",
    "    past_queries = memory.get(\"past_queries\", [])\n",
    "    past_queries.append(original_query)\n",
    "    memory[\"past_queries\"] = past_queries[-10:]  # Retain last 10 queries\n",
    "    state[\"memory\"] = memory\n",
    "\n",
    "    return state\n",
    "\n",
    "# === Node: Routing ===\n",
    "\n",
    "def router_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    original_query = state.get(\"original_query\", \"\")\n",
    "\n",
    "    # --- Phase 0: Detect possible loop/stuck condition ---\n",
    "    memory = state.get(\"memory\", {})\n",
    "    past_queries = memory.get(\"past_queries\", [])\n",
    "\n",
    "    # Simple stuck detection: repeated query threshold (e.g., 3 times)\n",
    "    if past_queries.count(original_query) >= 3:\n",
    "        state[\"intent\"] = \"meta-intervention\"\n",
    "        state.setdefault(\"reasoning_trace\", []).append(\n",
    "            f\"Loop detected: '{original_query}' appeared {past_queries.count(original_query)} times. Escalating to meta-intervention.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    # --- Phase 1: Extract structured query from orchestrator ---\n",
    "    route_result = orchestrator.route_to_agents(original_query)\n",
    "    \n",
    "    structured_query = {\n",
    "        \"intent\": route_result.get(\"intent\", \"reasoning\"),\n",
    "        \"sub_intents\": route_result.get(\"sub_intents\", []),\n",
    "        \"input_references\": route_result.get(\"input_references\", []),\n",
    "        \"reasoning_style\": route_result.get(\"reasoning_style\", \"default\"),\n",
    "        \"preferred_agents\": route_result.get(\"preferred_agents\", []),\n",
    "        \"metadata_flags\": route_result.get(\"metadata_flags\", {})\n",
    "    }\n",
    "\n",
    "    state[\"structured_query\"] = structured_query\n",
    "    state[\"intent\"] = structured_query[\"intent\"]\n",
    "    state.setdefault(\"reasoning_trace\", []).append(\n",
    "        f\"Structured query generated: {structured_query}\"\n",
    "    )\n",
    "\n",
    "    # --- Phase 2: Dispatch to appropriate backend (LangGraph, CrewAI, AutoGen) ---\n",
    "    dispatch_result = dispatch_query(structured_query, memory=state.get(\"memory\", {}))\n",
    "    state[\"selected_agents\"] = dispatch_result[\"selected_agents\"]\n",
    "    state[\"selected_backend\"] = dispatch_result[\"selected_backend\"]\n",
    "    state.setdefault(\"reasoning_trace\", []).extend([\n",
    "        f\"Dispatched to backend: {dispatch_result['selected_backend']} with agents: {dispatch_result['selected_agents']}\",\n",
    "        f\"Dispatch reasoning: {dispatch_result['dispatch_reason']}\"\n",
    "    ])\n",
    "\n",
    "    return state\n",
    "\n",
    "async def run_concurrent_orchestrators(user_query: str, metadata: dict):\n",
    "    # Create async tasks for both orchestrators\n",
    "    crewai_task = asyncio.create_task(crewAI_orchestrator.run(user_query=user_query, metadata=metadata))\n",
    "    autogen_task = asyncio.create_task(autogen_conversational_orchestrator.run(user_query=user_query, metadata=metadata))\n",
    "\n",
    "    # Wait for both tasks to complete\n",
    "    crew_output = await crewai_task\n",
    "    autogen_output = await autogen_task\n",
    "\n",
    "    return crew_output, autogen_output\n",
    "def execution_bridge_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls both AutoGen and CrewAI orchestrators with the user query and metadata.\n",
    "    Captures their outputs and stores them in the LangGraph state for later use.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "\n",
    "    # Run both orchestrators concurrently\n",
    "    try:\n",
    "        crew_output, autogen_output = asyncio.run(run_concurrent_orchestrators(user_query, metadata))\n",
    "    except Exception as e:\n",
    "        crew_output = f\"[Error from CrewAI]: {str(e)}\"\n",
    "        autogen_output = f\"[Error from AutoGen]: {str(e)}\"\n",
    "\n",
    "    # --- Step 3: Return updated state ---\n",
    "    return {\n",
    "        **state,\n",
    "        \"crew_output\": crew_output,\n",
    "        \"autogen_output\": autogen_output,\n",
    "        \"agent_outputs\": [str(crew_output), str(autogen_output)],\n",
    "        \"last_completed\": \"execution_bridge\"\n",
    "    }\n",
    "\n",
    "\n",
    "# === Helper to run agents ===\n",
    "def run_agent_if_intent_matches(state, valid_intents, agent, agent_name):\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    if intent in valid_intents:\n",
    "        response = agent.run(state[\"structured_query\"])\n",
    "        state.setdefault(\"agent_outputs\", []).append(response)\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"{agent_name} executed for intent: {intent}\")\n",
    "    return state\n",
    "\n",
    "# === Agent Nodes ===\n",
    "def competition_summary_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"data\", \"overview\"], competition_summary_agent, \"CompetitionSummaryAgent\")\n",
    "\n",
    "def notebook_explainer_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"code\", \"model\"], notebook_explainer_agent, \"NotebookExplainerAgent\")\n",
    "\n",
    "def discussion_helper_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"discussion\"], discussion_helper_agent, \"DiscussionHelperAgent\")\n",
    "\n",
    "def error_diagnosis_node(state): \n",
    "    return run_agent_if_intent_matches(state, [\"error\", \"bug\", \"troubleshooting\"], error_diagnosis_agent, \"ErrorDiagnosisAgent\")\n",
    "\n",
    "def reasoning_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"\n",
    "    Black-box node that wraps CrewAI multi-agent reasoning.\n",
    "    Expects state to contain 'original_query' and optionally 'metadata'.\n",
    "    Returns updated state with 'crew_result'.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"original_query\", \"\")\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    try:\n",
    "        result = crewAI_orchestrator.run(user_query, metadata)\n",
    "        state[\"crew_result\"] = result\n",
    "        state.setdefault(\"reasoning_trace\", []).append(\"Multi-agent reasoning node executed.\")\n",
    "        state.setdefault(\"agent_outputs\", []).append(result)\n",
    "    except Exception as e:\n",
    "        state[\"crew_result\"] = {\"error\": str(e)}\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"Multi-agent reasoning node failed: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "def conversational_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    \"\"\"\n",
    "    Node that wraps the AutoGen conversational orchestrator.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"original_query\", \"\")\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    try:\n",
    "        result = autogen_conversational_orchestrator.run(user_query, metadata)\n",
    "        state.setdefault(\"agent_outputs\", []).append(result)\n",
    "        state.setdefault(\"conversation_trace\", []).append(\"AutoGen conversational agent executed\")\n",
    "    except Exception as e:\n",
    "        state.setdefault(\"agent_outputs\", []).append({\"error\": str(e)})\n",
    "        state.setdefault(\"conversation_trace\", []).append(f\"AutoGen conversational agent failed: {str(e)}\")\n",
    "    return state\n",
    "\n",
    "def memory_update_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    memory = state.setdefault(\"memory\", {})\n",
    "    \n",
    "    # Update past queries\n",
    "    original_query = state.get(\"original_query\", \"\")\n",
    "    memory.setdefault(\"past_queries\", []).append(original_query)\n",
    "\n",
    "    # Track agent outputs\n",
    "    agent_outputs = state.get(\"agent_outputs\", [])\n",
    "    memory.setdefault(\"agent_outputs_log\", []).append(agent_outputs)\n",
    "\n",
    "    # Append reasoning trace\n",
    "    reasoning = state.get(\"reasoning_trace\", [])\n",
    "    memory.setdefault(\"reasoning_trace_log\", []).append(reasoning)\n",
    "\n",
    "    # Track intent history\n",
    "    current_intent = state.get(\"intent\", \"unknown\")\n",
    "    memory.setdefault(\"intent_history\", []).append(current_intent)\n",
    "\n",
    "    # Save back into state\n",
    "    state[\"memory\"] = memory\n",
    "    return state\n",
    "def meta_monitor_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    memory = state.get(\"memory\", {})\n",
    "    past_queries = memory.get(\"past_queries\", [])\n",
    "    agent_outputs_log = memory.get(\"agent_outputs_log\", [])\n",
    "    intent_history = memory.get(\"intent_history\", [])\n",
    "    \n",
    "    # Heuristics for intervention\n",
    "    too_repetitive = len(past_queries) >= 3 and len(set(past_queries[-3:])) == 1\n",
    "    no_output_recently = any(len(batch) == 0 for batch in agent_outputs_log[-2:])\n",
    "    switching_intents = len(set(intent_history[-3:])) > 2\n",
    "\n",
    "    needs_intervention = too_repetitive or no_output_recently or switching_intents\n",
    "    state.setdefault(\"reasoning_trace\", []).append(\n",
    "        f\"[MetaMonitor] Intervention needed? {needs_intervention} | Repetitive: {too_repetitive}, Output Missing: {no_output_recently}, Switching Intents: {switching_intents}\"\n",
    "    )\n",
    "    state[\"meta_intervention_needed\"] = needs_intervention\n",
    "    return state\n",
    "def meta_intervention_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    agent = ProgressMonitorAgent().get_crew_agent()\n",
    "\n",
    "    # Create a mock task for intervention\n",
    "    task = Task(\n",
    "        name=\"Meta-Intervention Analysis\",\n",
    "        description=\"Diagnose why recent runs have failed or stalled. Recommend a different approach, reasoning style, or agent mix.\",\n",
    "        agent=agent,\n",
    "        context={\"memory\": state.get(\"memory\", {})},\n",
    "        expected_output=\"Actionable diagnosis and re-strategizing\"\n",
    "    )\n",
    "\n",
    "    crew = Crew(agents=[agent], tasks=[task], process=\"sequential\")\n",
    "    try:\n",
    "        output = crew.kickoff()\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"[MetaIntervention] {output}\")\n",
    "        state[\"agent_outputs\"].append({\"agent_name\": \"ProgressMonitor\", \"response\": output})\n",
    "    except Exception as e:\n",
    "        state.setdefault(\"reasoning_trace\", []).append(f\"[MetaIntervention] Failed: {e}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def scoring_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    from langchain.chains import LLMChain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    scoring_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"You are evaluating the quality of an assistant's response.\n",
    "        \n",
    "        Response:\n",
    "        {response}\n",
    "\n",
    "        Rate the response on a scale of 0 to 100 based on:\n",
    "        - Relevance to the user's query\n",
    "        - Specificity and usefulness\n",
    "        - Clarity and structure\n",
    "\n",
    "        Only return a number.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    scoring_chain = LLMChain(llm=ChatOpenAI(temperature=0.0), prompt=scoring_prompt)\n",
    "\n",
    "    scored_outputs = []\n",
    "    for output in state.get(\"agent_outputs\", []):\n",
    "        try:\n",
    "            score_str = scoring_chain.run(response=output[\"response\"])\n",
    "            score = int(score_str.strip())\n",
    "            output[\"score\"] = score\n",
    "            state.setdefault(\"reasoning_trace\", []).append(\n",
    "                f\"Scored agent {output['agent_name']} with {score}/100\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            output[\"score\"] = 50  # Fallback\n",
    "            state.setdefault(\"reasoning_trace\", []).append(\n",
    "                f\"Scoring failed for {output['agent_name']}: {e}\"\n",
    "            )\n",
    "        scored_outputs.append(output)\n",
    "\n",
    "    state[\"agent_outputs\"] = scored_outputs\n",
    "    return state\n",
    "\n",
    "\n",
    "# === Aggregation Node ===\n",
    "def aggregation_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    responses = state.get(\"agent_outputs\", [])\n",
    "    if not responses:\n",
    "        state[\"final_response\"] = \"No relevant agent responses to aggregate.\"\n",
    "        return state\n",
    "    final = orchestrator.aggregate_response(responses)\n",
    "    state[\"final_response\"] = final\n",
    "    return state\n",
    "\n",
    "# === Intent Routing Function ===\n",
    "def route_by_intent(state: OrchestratorState) -> str:\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    router_dict = {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\",\n",
    "        \"progress\": \"reasoning_node\",\n",
    "        \"strategy\": \"reasoning_node\",\n",
    "        \"multi-hop\": \"reasoning_node\",\n",
    "        \"conversational\": \"conversational_node\"  # NEW for AutoGen\n",
    "    }\n",
    "    return router_dict.get(intent, \"reasoning_node\")  # Fallback\n",
    "# Add Edge: Start → Preprocessing → Router → Execution Bridge\n",
    "baseGraph.add_edge(START, \"preprocessing\")\n",
    "baseGraph.add_edge(\"preprocessing\", \"router\")\n",
    "baseGraph.add_edge(\"router\", \"execution_bridge\")\n",
    "\n",
    "# Execution Bridge → conditional dispatch\n",
    "baseGraph.add_conditional_edges(\n",
    "    \"execution_bridge\",\n",
    "    route_by_intent,\n",
    "    {\n",
    "        \"overview\": \"competition_summary\",\n",
    "        \"data\": \"competition_summary\",\n",
    "        \"code\": \"notebook_explainer\",\n",
    "        \"model\": \"notebook_explainer\",\n",
    "        \"discussion\": \"discussion_helper\",\n",
    "        \"error\": \"error_diagnosis\",\n",
    "        \"bug\": \"error_diagnosis\",\n",
    "        \"troubleshooting\": \"error_diagnosis\",\n",
    "        \"progress\": \"reasoning_node\",\n",
    "        \"strategy\": \"reasoning_node\",\n",
    "        \"multi-hop\": \"reasoning_node\",\n",
    "        \"conversational\": \"conversational_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Domain-specific nodes → memory update\n",
    "baseGraph.add_edge(\"competition_summary\", \"memory_update\")\n",
    "baseGraph.add_edge(\"notebook_explainer\", \"memory_update\")\n",
    "baseGraph.add_edge(\"discussion_helper\", \"memory_update\")\n",
    "baseGraph.add_edge(\"error_diagnosis\", \"memory_update\")\n",
    "baseGraph.add_edge(\"reasoning_node\", \"memory_update\")\n",
    "baseGraph.add_edge(\"conversational_node\", \"memory_update\")\n",
    "\n",
    "# Memory Update → Meta-Monitor\n",
    "baseGraph.add_edge(\"memory_update\", \"meta_monitor\")\n",
    "\n",
    "# Meta-Monitor → Meta-Intervention or Skip\n",
    "baseGraph.add_conditional_edges(\n",
    "    \"meta_monitor\",\n",
    "    lambda state: \"meta_intervention\" if state.get(\"meta_intervention_needed\") else \"scoring_node\",\n",
    "    {\n",
    "        \"meta_intervention\": \"meta_intervention\",\n",
    "        \"scoring_node\": \"scoring_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Meta-Intervention → Scoring\n",
    "baseGraph.add_edge(\"meta_intervention\", \"scoring_node\")\n",
    "\n",
    "# Scoring → Aggregation → END\n",
    "baseGraph.add_edge(\"scoring_node\", \"aggregation\")\n",
    "baseGraph.add_edge(\"aggregation\", END)\n",
    "\n",
    "# === Compile Graph ===\n",
    "compiled_graph = baseGraph.compile(checkpointer=memory)\n",
    "\n",
    "# Optional visualization\n",
    "compiled_graph.visualize(path=\"my_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import threading\n",
    "from typing import Optional, Dict, Any\n",
    "import json\n",
    "\n",
    "import autogen\n",
    "from autogen import GroupChat, GroupChatManager, UserProxyAgent, AssistantAgent\n",
    "\n",
    "from null.multi_hop_agent import MultiHopReasoningAgent\n",
    "from null.progress_monitor_agent import ProgressMonitorAgent\n",
    "from null.code_feedback_agent import CodeFeedbackAgent\n",
    "from null.timeline_coach_agent import TimelineCoachAgent\n",
    "\n",
    "\n",
    "class AutoGenReasoningOrchestrator:\n",
    "    def __init__(self):\n",
    "        # Step 1: LLM config (can be extended to use environment config)\n",
    "        self.llm_config = {\n",
    "            \"timeout\": 600,\n",
    "            \"seed\": 42,\n",
    "            \"config_list\": autogen.config_list_from_json(\n",
    "                \"OAI_CONFIG_LIST\",\n",
    "                filter_dict={\"model\": [\"gpt-4\", \"gpt-3.5-turbo\"]},\n",
    "            ),\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "\n",
    "        # Step 2: Load and wrap all agents using .get_autogen_agent()\n",
    "        self.user_proxy = UserProxyAgent(\n",
    "            name=\"Admin\",\n",
    "            system_message=\"You are the human coordinator. Say 'TERMINATE' to end.\",\n",
    "            code_execution_config={\"use_docker\": False, \"work_dir\": \"coding\"},\n",
    "            human_input_mode=\"ALWAYS\",\n",
    "            llm_config=self.llm_config,\n",
    "        )\n",
    "\n",
    "        self.multi_hop_agent = MultiHopReasoningAgent().get_autogen_agent(self.llm_config)\n",
    "        self.progress_monitor_agent = ProgressMonitorAgent().get_autogen_agent(self.llm_config)\n",
    "        self.code_feedback_agent = CodeFeedbackAgent().get_autogen_agent(self.llm_config)\n",
    "        self.timeline_coach_agent = TimelineCoachAgent().get_autogen_agent(self.llm_config)\n",
    "        self.round_counter = 0\n",
    "        self.agent_add_history = []\n",
    "        self.recent_discussion = []\n",
    "    def build_groupchats(self) -> Dict[str, GroupChatManager]:\n",
    "        \"\"\"\n",
    "        Create modular group chats for each major task.\n",
    "        \"\"\"\n",
    "        # Group 1: Code review\n",
    "        code_chat = GroupChat(\n",
    "            agents=[self.user_proxy, self.code_feedback_agent],\n",
    "            messages=[],\n",
    "            max_round=10,\n",
    "        )\n",
    "        code_manager = GroupChatManager(groupchat=code_chat, llm_config=self.llm_config)\n",
    "\n",
    "        # Group 2: Progress tracking and timeline coaching\n",
    "        progress_chat = GroupChat(\n",
    "            agents=[self.user_proxy, self.progress_monitor_agent, self.timeline_coach_agent],\n",
    "            messages=[],\n",
    "            max_round=10,\n",
    "        )\n",
    "        progress_manager = GroupChatManager(groupchat=progress_chat, llm_config=self.llm_config)\n",
    "\n",
    "        # Group 3: Multi-hop reasoning\n",
    "        reasoning_chat = GroupChat(\n",
    "            agents=[self.user_proxy, self.multi_hop_agent],\n",
    "            messages=[],\n",
    "            max_round=10,\n",
    "        )\n",
    "        reasoning_manager = GroupChatManager(groupchat=reasoning_chat, llm_config=self.llm_config)\n",
    "\n",
    "        return {\n",
    "            \"code_review\": code_manager,\n",
    "            \"progress_eval\": progress_manager,\n",
    "            \"reasoning\": reasoning_manager,\n",
    "        }\n",
    "\n",
    "    def custom_speaker_selection(self, last_speaker, last_message, groupchat, selector):\n",
    "        self.round_counter += 1\n",
    "        cooldown_window = 3  # how many rounds to avoid re-adding an agent\n",
    "\n",
    "        last_message_content = last_message.get(\"content\", \"\")\n",
    "        last_speaker_name = last_message.get(\"name\", \"\")\n",
    "        print(f\"\\n[DEBUG] ⏱ Round {self.round_counter}\")\n",
    "        print(f\"[DEBUG] Last message from '{last_speaker_name}': {last_message_content}\")\n",
    "\n",
    "        current_agents = [agent.name for agent in groupchat.agents]\n",
    "        print(f\"[DEBUG] Current agents: {current_agents}\")\n",
    "\n",
    "        agent_pool = {\n",
    "            \"MultiHopReasoningAgent\": self.multi_hop_agent,\n",
    "            \"ProgressMonitorAgent\": self.progress_monitor_agent,\n",
    "            \"CodeFeedbackAgent\": self.code_feedback_agent,\n",
    "            \"TimelineCoachAgent\": self.timeline_coach_agent,\n",
    "        }\n",
    "\n",
    "        # Step 1: Classify message intent\n",
    "        # Use self.llm_config instead of undefined llm_config\n",
    "        llm_prompt = (\n",
    "            \"Classify the intent of the following message and extract any relevant entities.\\n\"\n",
    "            f\"Message: {last_message_content}\\n\"\n",
    "            \"Respond as JSON: {\\\"intent\\\": ..., \\\"entities\\\": [...]}.\"\n",
    "        )\n",
    "        try:\n",
    "            llm_response = autogen.oai.ChatCompletion.create(\n",
    "                model=self.llm_config[\"config_list\"][0][\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": llm_prompt}],\n",
    "                api_key=self.llm_config[\"config_list\"][0].get(\"api_key\"),\n",
    "                api_base=self.llm_config[\"config_list\"][0].get(\"api_base\"),\n",
    "                temperature=self.llm_config.get(\"temperature\", 0.7),\n",
    "                timeout=self.llm_config.get(\"timeout\", 600),\n",
    "            )\n",
    "            llm_content = llm_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            parsed = json.loads(llm_content)\n",
    "            intent = parsed.get(\"intent\", \"\").strip().lower()\n",
    "            entities = [e.lower() for e in parsed.get(\"entities\", [])]\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] LLM parsing failed: {e}\")\n",
    "            intent = \"\"\n",
    "            entities = []\n",
    "\n",
    "        print(f\"[DEBUG] LLM intent: {intent}, entities: {entities}\")\n",
    "\n",
    "        # Step 2: Guard against missing intent\n",
    "        if not intent:\n",
    "            print(\"[INFO] No clear intent. Skipping agent update.\")\n",
    "            return autogen.agentchat.groupchat.GroupChat.auto_select_speaker(\n",
    "                last_speaker, selector, selector.groupchat.messages, selector.groupchat.agents\n",
    "            )\n",
    "\n",
    "        # Step 3: Get required agents\n",
    "        intent_agent_map = {\n",
    "            \"code_review\": [\"CodeFeedbackAgent\"],\n",
    "            \"progress_tracking\": [\"ProgressMonitorAgent\", \"TimelineCoachAgent\"],\n",
    "            \"multi_hop_reasoning\": [\"MultiHopReasoningAgent\"],\n",
    "            \"timeline_coaching\": [\"TimelineCoachAgent\"],\n",
    "        }\n",
    "        required_agents = intent_agent_map.get(intent, [])\n",
    "\n",
    "        # Step 4: Remove agents not needed\n",
    "        for agent in groupchat.agents[:]:  # Copy to avoid mutation during loop\n",
    "            if agent.name != \"Admin\" and agent.name not in required_agents:\n",
    "                print(f\"--- Removing {agent.name} from the group chat (task complete) ---\")\n",
    "                groupchat.agents.remove(agent)\n",
    "\n",
    "        # Step 5: Add needed agents if not throttled\n",
    "        for agent_name in required_agents:\n",
    "            recently_added = any(\n",
    "                (record[0] == agent_name and record[1] == intent and self.round_counter - record[2] <= cooldown_window)\n",
    "                for record in self.agent_add_history\n",
    "            )\n",
    "\n",
    "            if agent_name not in current_agents:\n",
    "                if recently_added:\n",
    "                    print(f\"[INFO] ⛔ Skipping {agent_name} (recently added for intent '{intent}')\")\n",
    "                elif agent_name in agent_pool:\n",
    "                    agent_obj = agent_pool[agent_name]\n",
    "                    print(f\"--- ✅ Adding {agent_name} to the group chat for intent '{intent}' ---\")\n",
    "                    groupchat.agents.append(agent_obj)\n",
    "                    self.agent_add_history.append((agent_name, intent, self.round_counter))\n",
    "\n",
    "        # Step 6: Fallback if Admin is alone\n",
    "        if len(groupchat.agents) == 1:\n",
    "            print(\"[WARN] Only Admin left. Re-adding MultiHopReasoningAgent.\")\n",
    "            groupchat.agents.append(self.multi_hop_agent)\n",
    "\n",
    "        # Step 7: Print agents considered for speaker selection\n",
    "        final_candidates = [agent.name for agent in groupchat.agents]\n",
    "        print(f\"[DEBUG] 🗣 Speaker candidates for auto_select: {final_candidates}\")\n",
    "# Track recent discussion\n",
    "        if last_speaker_name and intent:\n",
    "            self.recent_discussion.append((self.round_counter, last_speaker_name, intent))\n",
    "            self.recent_discussion = self.recent_discussion[-10:]  # Limit memory\n",
    "\n",
    "# Reorder agents to prioritize recent intent responders\n",
    "        prioritized_agents = sorted(\n",
    "        groupchat.agents,\n",
    "    key=lambda agent: next(\n",
    "        (10 - (self.round_counter - rd[0])  # Higher score for recent\n",
    "         for rd in reversed(self.recent_discussion)\n",
    "         if rd[1] == agent.name and rd[2] == intent),\n",
    "        0\n",
    "    ),\n",
    "    reverse=True,\n",
    ")\n",
    "        print(f\"[DEBUG] Prioritized agent order: {[agent.name for agent in prioritized_agents]}\")\n",
    "        selector.groupchat.agents = prioritized_agents\n",
    "\n",
    "        return autogen.agentchat.groupchat.GroupChat.auto_select_speaker(\n",
    "    last_speaker, selector, selector.groupchat.messages, selector.groupchat.agents\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, user_query: str, metadata: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Main method to coordinate parallel AutoGen agent groups.\n",
    "        \"\"\"\n",
    "        group_managers = self.build_groupchats()\n",
    "\n",
    "        thread_tasks = [\n",
    "            (group_managers[\"code_review\"], \"Please review the user's model code.\"),\n",
    "            (group_managers[\"progress_eval\"], \"Assess the user's progress and suggest improvements.\"),\n",
    "            (group_managers[\"reasoning\"], f\"Answer the user's query: {user_query}\"),\n",
    "        ]\n",
    "\n",
    "        threads = []\n",
    "\n",
    "        for manager, message in thread_tasks:\n",
    "            t = threading.Thread(target=self.user_proxy.initiate_chat, args=(manager, message))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        print(\"\\n✅ All group chats completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb58f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "from crewai import Crew, Agent, Task, Process\n",
    "from textwrap import dedent\n",
    "from typing import Optional, Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "# === Reasoning Agents ===\n",
    "from null.multi_hop_reasoning import MultiHopReasoningAgent\n",
    "from null.progress_monitor import ProgressMonitorAgent\n",
    "from null.code_feedback import CodeFeedbackAgent\n",
    "from null.timeline_coach import TimelineCoachAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c10684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Orchestrator ===\n",
    "class MultiAgentReasoningOrchestrator:\n",
    "    def __init__(self, agents: Optional[List[CrewAIAgent]] = None):\n",
    "        if agents:\n",
    "            self.agents = agents\n",
    "        else:\n",
    "            # Default crew agents\n",
    "            self.multi_hop_agent = MultiHopReasoningAgent().get_crew_agent()\n",
    "            self.progress_monitor_agent = ProgressMonitorAgent().get_crew_agent()\n",
    "            self.code_feedback_agent = CodeFeedbackAgent().get_crew_agent()\n",
    "            self.timeline_coach_agent = TimelineCoachAgent().get_crew_agent()\n",
    "\n",
    "            self.agents = [\n",
    "                self.multi_hop_agent,\n",
    "                self.progress_monitor_agent,\n",
    "                self.code_feedback_agent,\n",
    "                self.timeline_coach_agent\n",
    "            ]\n",
    "\n",
    "    def build_tasks(self, user_query: str, metadata: Optional[Dict[str, Any]] = None) -> List[Task]:\n",
    "        \"\"\"\n",
    "        Define the reasoning tasks for the agents to work on,\n",
    "        with context-sensitive prompts based on the user query and metadata.\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            Task(\n",
    "                name=\"Meta Strategy Coordination\",\n",
    "                description=\"Analyze the user’s rank and recent interactions. Recommend strategic adjustments for the rest of the team based on progress gaps or inefficiencies.\",\n",
    "                agent=self.progress_monitor_agent,\n",
    "                expected_output=\"A meta-strategy suggestion that guides the agent team toward better support strategies.\",\n",
    "                context={\"query\": user_query, \"metadata\": metadata}\n",
    "            ),\n",
    "            Task(\n",
    "                name=\"Multi-Hop Reasoning\",\n",
    "                description=\"Coordinate several expert agents to resolve a complex or multi-part user query. Organize sub-questions if necessary and synthesize a coherent final response.\",\n",
    "                agent=self.multi_hop_agent,\n",
    "                expected_output=\"A step-by-step explanation showing how insights were aggregated across reasoning steps.\",\n",
    "                context={\"query\": user_query, \"metadata\": metadata}\n",
    "            ),\n",
    "            Task(\n",
    "                name=\"Code Evaluation\",\n",
    "                description=\"Evaluate the user's code for logic, performance, and best practices. Offer actionable feedback and improvement tips without copying from others.\",\n",
    "                agent=self.code_feedback_agent,\n",
    "                expected_output=\"A structured review of the code with strengths, weaknesses, and improvement guidance.\",\n",
    "                context={\"query\": user_query, \"metadata\": metadata}\n",
    "            ),\n",
    "            Task(\n",
    "                name=\"Timeline Guidance\",\n",
    "                description=\"Assess the user's progress timeline and advise on any bottlenecks, neglected phases, or timing risks. Propose pacing strategies to stay on schedule.\",\n",
    "                agent=self.timeline_coach_agent,\n",
    "                expected_output=\"A checklist-style plan marking completed vs pending milestones with suggested adjustments.\",\n",
    "                context={\"query\": user_query, \"metadata\": metadata}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return tasks\n",
    "    \n",
    "    def compose_crew(self, tasks: List[Task]) -> Crew:\n",
    "        \"\"\"\n",
    "        Initialize a CrewAI Crew object with agents, tasks, and coordination logic.\n",
    "        \"\"\"\n",
    "        # Use self.code_feedback_agent instead of undefined code_feedback_agent\n",
    "        multiagent_reasoning_crew = Crew(agents=self.agents, tasks=tasks, process=Process.hierarchial)\n",
    "        return multiagent_reasoning_crew\n",
    "    \n",
    "    def run(self, user_query: str, metadata: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "    High-level method to execute reasoning flow from user query.\n",
    "    1. Builds tasks\n",
    "    2. Composes crew\n",
    "    3. Runs the crew\n",
    "    4. Returns the final result\n",
    "    \"\"\"\n",
    "        logger.info(f\"Starting reasoning for user query: {user_query}\")\n",
    "        if metadata:\n",
    "            logger.debug(f\"Metadata: {metadata}\")\n",
    "\n",
    "    # Step 1: Build tasks\n",
    "        tasks = self.build_tasks(user_query, metadata)\n",
    "        logger.info(f\"Generated {len(tasks)} tasks for reasoning agents.\")\n",
    "\n",
    "    # Step 2: Compose the crew with selected agents and strategy\n",
    "        crew = self.compose_crew(tasks)\n",
    "        logger.info(\"Crew composed and ready for kickoff.\")\n",
    "    # Step 3: Run the crew — agents execute tasks in hierarchical coordination\n",
    "        try:\n",
    "            result = crew.kickoff(verbose=True)\n",
    "            logger.info(\"Crew execution completed successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Crew execution failed: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    # Step 4: Return the aggregated reasoning output\n",
    "        logger.debug(f\"Aggregated reasoning output: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90927771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
